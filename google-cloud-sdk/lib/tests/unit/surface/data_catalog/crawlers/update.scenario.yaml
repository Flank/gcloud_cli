title: Data Catalog update crawler scenario test.
release_tracks: [ALPHA]
summary:
# This summary is generated automatically on update and should not be edited.
- execute:
  - label: Simple update (display name and description only)
  - command: data-catalog crawlers update crawler1 --display-name=name1 --description=description1
  - stderr: |
      Updated crawler [crawler1].
  - stdout: |
      config:
        adHocRun: {}
        projectScope: {}
      description: description1
      displayName: name1
      name: projects/fake-project/crawlers/crawler1
- execute:
  - label: Add and remove bundle specs
  - command: data-catalog crawlers update crawler1 --remove-bundle-specs=dir1/subdir1
      --add-bundle-specs=dir2/subdir2,dir3/subdir3
  - stderr: |
      Updated crawler [crawler1].
  - stdout: |
      config:
        adHocRun: {}
        bundleSpecs:
        - dir2/subdir2
        - dir3/subdir3
        projectScope: {}
      name: projects/fake-project/crawlers/crawler1
- execute:
  - label: Clear bundle specs
  - command: data-catalog crawlers update crawler1 --clear-bundle-specs
  - stderr: |
      Updated crawler [crawler1].
  - stdout: |
      config:
        adHocRun: {}
        projectScope: {}
      name: projects/fake-project/crawlers/crawler1
- execute:
  - label: Add and remove buckets
  - command: data-catalog crawlers update crawler1 --remove-buckets=gs://bucket1 --add-buckets=gs://bucket2,gs://bucket3
  - stderr: |
      Updated crawler [crawler1].
  - stdout: |
      config:
        adHocRun: {}
        bucketScope:
          buckets:
          - bucket: gs://bucket2
          - bucket: gs://bucket3
      name: projects/fake-project/crawlers/crawler1
- execute:
  - label: Clear and add buckets
  - command: data-catalog crawlers update crawler1 --clear-buckets --add-buckets=gs://bucket2
  - stderr: |
      Updated crawler [crawler1].
  - stdout: |
      config:
        adHocRun: {}
        bucketScope:
          buckets:
          - bucket: gs://bucket2
      name: projects/fake-project/crawlers/crawler1
- execute:
  - label: Change crawl scope to organization
  - command: data-catalog crawlers update crawler1 --crawl-scope=organization
  - stderr: |
      Updated crawler [crawler1].
  - stdout: |
      config:
        adHocRun: {}
        organizationScope: {}
      name: projects/fake-project/crawlers/crawler1
- execute:
  - label: Change run option to manual
  - command: data-catalog crawlers update crawler1 --run-option=manual
  - stderr: |
      Updated crawler [crawler1].
  - stdout: |
      config:
        adHocRun: {}
        organizationScope: {}
      name: projects/fake-project/crawlers/crawler1
- execute:
  - label: Update everything
  - command: data-catalog crawlers update crawler1 --display-name=name1 --description=description1
      --run-option=scheduled --run-schedule=weekly --crawl-scope=bucket --add-buckets=gs://bucket1
      --clear-bundle-specs
  - stderr: |
      Updated crawler [crawler1].
  - stdout: |
      config:
        bucketScope:
          buckets:
          - bucket: gs://bucket1
        scheduledRun:
          scheduledRunOption: WEEKLY
      description: description1
      displayName: name1
      name: projects/fake-project/crawlers/crawler1
- execute:
  - label: Bucket crawl scope specified without buckets to add or remove
  - command: data-catalog crawlers update crawler1 --crawl-scope=bucket
  - error: '1: Must provide buckets to add or remove when updating the crawl scope
      of a bucket-scoped crawler.'
- execute:
  - label: Buckets modified for non-bucket scoped crawler (scope explicitly specified)
  - command: data-catalog crawlers update crawler1 --add-buckets=gs://bucket1 --crawl-scope=project
  - error: '1: Arguments `--add-buckets`, `--remove-buckets`, and `--clear-buckets`
      are only valid for bucket-scoped crawlers. Use `--crawl-scope=bucket` to specify
      a bucket-scoped crawler.'
- execute:
  - label: Buckets modified for non-bucket scoped crawler (scope determined by GET
      request)
  - command: data-catalog crawlers update crawler1 --add-buckets=gs://bucket1
  - error: '1: Arguments `--add-buckets`, `--remove-buckets`, and `--clear-buckets`
      are only valid for bucket-scoped crawlers. Use `--crawl-scope=bucket` to specify
      a bucket-scoped crawler.'
- execute:
  - label: Run schedule provided for non-scheduled crawler (run option explicitly
      specified)
  - command: data-catalog crawlers update crawler1 --run-option=manual --run-schedule=daily
  - error: '1: Argument `--run-schedule` can only be provided for scheduled crawlers.
      Use `--run-option=scheduled` to specify a scheduled crawler.'
- execute:
  - label: Run schedule provided for non-scheduled crawler (run option determined
      by GET request)
  - command: data-catalog crawlers update crawler1 --run-schedule=daily
  - error: '1: Argument `--run-schedule` can only be provided for scheduled crawlers.
      Use `--run-option=scheduled` to specify a scheduled crawler.'
- execute:
  - label: No fields specified
  - command: data-catalog crawlers update crawler1
  - error: '1: Must specify at least one parameter to update.'
actions:
- execute_command:
    label: Simple update (display name and description only)
    command: data-catalog crawlers update crawler1 --display-name=name1 --description=description1
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json&updateMask=description%2CdisplayName
          method: PATCH
          headers: {}
          body:
            json:
              description: description1
              displayName: name1
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "displayName": "name1",
              "description": "description1",
              "config": {
                "projectScope": {},
                "adHocRun": {}
              }
            }
    - expect_stderr: |
        Updated crawler [crawler1].
    - expect_stdout: |
        config:
          adHocRun: {}
          projectScope: {}
        description: description1
        displayName: name1
        name: projects/fake-project/crawlers/crawler1
    - expect_exit:
        code: 0
- execute_command:
    label: Add and remove bundle specs
    command: data-catalog crawlers update crawler1 --remove-bundle-specs=dir1/subdir1
      --add-bundle-specs=dir2/subdir2,dir3/subdir3
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json
          method: GET
          headers: {}
          body: null
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "projectScope": {},
                "adHocRun": {},
                "bundleSpecs": [
                  "dir1/subdir1"
                ]
              }
            }
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json&updateMask=config.bundleSpecs
          method: PATCH
          headers: {}
          body:
            json:
              config:
                bundleSpecs:
                - dir2/subdir2
                - dir3/subdir3
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "projectScope": {},
                "adHocRun": {},
                "bundleSpecs": [
                  "dir2/subdir2",
                  "dir3/subdir3"
                ]
              }
            }
    - expect_stderr: |
        Updated crawler [crawler1].
    - expect_stdout: |
        config:
          adHocRun: {}
          bundleSpecs:
          - dir2/subdir2
          - dir3/subdir3
          projectScope: {}
        name: projects/fake-project/crawlers/crawler1
    - expect_exit:
        code: 0
- execute_command:
    label: Clear bundle specs
    command: data-catalog crawlers update crawler1 --clear-bundle-specs
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json
          method: GET
          headers: {}
          body: null
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "projectScope": {},
                "adHocRun": {},
                "bundleSpecs": [
                  "dir1/subdir1"
                ]
              }
            }
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json&updateMask=config.bundleSpecs
          method: PATCH
          headers: {}
          body:
            json:
              config: {}
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "projectScope": {},
                "adHocRun": {}
              }
            }
    - expect_stderr: |
        Updated crawler [crawler1].
    - expect_stdout: |
        config:
          adHocRun: {}
          projectScope: {}
        name: projects/fake-project/crawlers/crawler1
    - expect_exit:
        code: 0
- execute_command:
    label: Add and remove buckets
    command: data-catalog crawlers update crawler1 --remove-buckets=gs://bucket1 --add-buckets=gs://bucket2,gs://bucket3
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json
          method: GET
          headers: {}
          body: null
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "bucketScope": {
                  "buckets": [
                    {
                      "bucket": "gs://bucket1"
                    }
                  ]
                },
                "adHocRun": {}
              }
            }
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json&updateMask=config.bucketScope
          method: PATCH
          headers: {}
          body:
            json:
              config:
                bucketScope:
                  buckets:
                  - bucket: gs://bucket2
                  - bucket: gs://bucket3
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "bucketScope": {
                  "buckets": [
                    {
                      "bucket": "gs://bucket2"
                    },
                    {
                      "bucket": "gs://bucket3"
                    }
                  ]
                },
                "adHocRun": {}
              }
            }
    - expect_stderr: |
        Updated crawler [crawler1].
    - expect_stdout: |
        config:
          adHocRun: {}
          bucketScope:
            buckets:
            - bucket: gs://bucket2
            - bucket: gs://bucket3
        name: projects/fake-project/crawlers/crawler1
    - expect_exit:
        code: 0
- execute_command:
    label: Clear and add buckets
    command: data-catalog crawlers update crawler1 --clear-buckets --add-buckets=gs://bucket2
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json
          method: GET
          headers: {}
          body: null
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "bucketScope": {
                  "buckets": [
                    {
                      "bucket": "gs://bucket1"
                    }
                  ]
                },
                "adHocRun": {}
              }
            }
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json&updateMask=config.bucketScope
          method: PATCH
          headers: {}
          body:
            json:
              config:
                bucketScope:
                  buckets:
                  - bucket: gs://bucket2
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "bucketScope": {
                  "buckets": [
                    {
                      "bucket": "gs://bucket2"
                    }
                  ]
                },
                "adHocRun": {}
              }
            }
    - expect_stderr: |
        Updated crawler [crawler1].
    - expect_stdout: |
        config:
          adHocRun: {}
          bucketScope:
            buckets:
            - bucket: gs://bucket2
        name: projects/fake-project/crawlers/crawler1
    - expect_exit:
        code: 0
- execute_command:
    label: Change crawl scope to organization
    command: data-catalog crawlers update crawler1 --crawl-scope=organization
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json&updateMask=config.organizationScope
          method: PATCH
          headers: {}
          body:
            json:
              config:
                organizationScope: {}
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "organizationScope": {},
                "adHocRun": {}
              }
            }
    - expect_stderr: |
        Updated crawler [crawler1].
    - expect_stdout: |
        config:
          adHocRun: {}
          organizationScope: {}
        name: projects/fake-project/crawlers/crawler1
    - expect_exit:
        code: 0
- execute_command:
    label: Change run option to manual
    command: data-catalog crawlers update crawler1 --run-option=manual
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json&updateMask=config.adHocRun
          method: PATCH
          headers: {}
          body:
            json:
              config:
                adHocRun: {}
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "organizationScope": {},
                "adHocRun": {}
              }
            }
    - expect_stderr: |
        Updated crawler [crawler1].
    - expect_stdout: |
        config:
          adHocRun: {}
          organizationScope: {}
        name: projects/fake-project/crawlers/crawler1
    - expect_exit:
        code: 0
- execute_command:
    label: Update everything
    command: data-catalog crawlers update crawler1 --display-name=name1 --description=description1
      --run-option=scheduled --run-schedule=weekly --crawl-scope=bucket --add-buckets=gs://bucket1
      --clear-bundle-specs
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json
          method: GET
          headers: {}
          body: null
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "organizationScope": {},
                "adHocRun": {},
                "bundleSpecs": [
                  "dir1/subdir1"
                ]
              }
            }
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json&updateMask=description%2CdisplayName%2Cconfig.bundleSpecs%2Cconfig.bucketScope%2Cconfig.scheduledRun
          method: PATCH
          headers: {}
          body:
            json:
              config:
                bucketScope:
                  buckets:
                  - bucket: gs://bucket1
                scheduledRun:
                  scheduledRunOption: WEEKLY
              description: description1
              displayName: name1
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "displayName": "name1",
              "description": "description1",
              "config": {
                "bucketScope": {
                  "buckets": [
                    {
                      "bucket": "gs://bucket1"
                    }
                  ]
                },
                "scheduledRun": {
                  "scheduledRunOption": "WEEKLY"
                }
              }
            }
    - expect_stderr: |
        Updated crawler [crawler1].
    - expect_stdout: |
        config:
          bucketScope:
            buckets:
            - bucket: gs://bucket1
          scheduledRun:
            scheduledRunOption: WEEKLY
        description: description1
        displayName: name1
        name: projects/fake-project/crawlers/crawler1
    - expect_exit:
        code: 0
- execute_command:
    label: Bucket crawl scope specified without buckets to add or remove
    command: data-catalog crawlers update crawler1 --crawl-scope=bucket
    events:
    - expect_exit:
        code: 1
        message: Must provide buckets to add or remove when updating the crawl scope
          of a bucket-scoped crawler.
- execute_command:
    label: Buckets modified for non-bucket scoped crawler (scope explicitly specified)
    command: data-catalog crawlers update crawler1 --add-buckets=gs://bucket1 --crawl-scope=project
    events:
    - expect_exit:
        code: 1
        message: Arguments `--add-buckets`, `--remove-buckets`, and `--clear-buckets`
          are only valid for bucket-scoped crawlers. Use `--crawl-scope=bucket` to
          specify a bucket-scoped crawler.
- execute_command:
    label: Buckets modified for non-bucket scoped crawler (scope determined by GET
      request)
    command: data-catalog crawlers update crawler1 --add-buckets=gs://bucket1
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json
          method: GET
          headers: {}
          body: null
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "projectScope": {},
                "adHocRun": {}
              }
            }
    - expect_exit:
        code: 1
        message: Arguments `--add-buckets`, `--remove-buckets`, and `--clear-buckets`
          are only valid for bucket-scoped crawlers. Use `--crawl-scope=bucket` to
          specify a bucket-scoped crawler.
- execute_command:
    label: Run schedule provided for non-scheduled crawler (run option explicitly
      specified)
    command: data-catalog crawlers update crawler1 --run-option=manual --run-schedule=daily
    events:
    - expect_exit:
        code: 1
        message: Argument `--run-schedule` can only be provided for scheduled crawlers.
          Use `--run-option=scheduled` to specify a scheduled crawler.
- execute_command:
    label: Run schedule provided for non-scheduled crawler (run option determined
      by GET request)
    command: data-catalog crawlers update crawler1 --run-schedule=daily
    events:
    - api_call:
        expect_request:
          uri: https://datacatalog.googleapis.com/v1alpha3/projects/fake-project/crawlers/crawler1?alt=json
          method: GET
          headers: {}
          body: null
        return_response:
          headers:
            status: '200'
          body: |
            {
              "name": "projects/fake-project/crawlers/crawler1",
              "config": {
                "projectScope": {},
                "adHocRun": {}
              }
            }
    - expect_exit:
        code: 1
        message: Argument `--run-schedule` can only be provided for scheduled crawlers.
          Use `--run-option=scheduled` to specify a scheduled crawler.
- execute_command:
    label: No fields specified
    command: data-catalog crawlers update crawler1
    events:
    - expect_exit:
        code: 1
        message: Must specify at least one parameter to update.
