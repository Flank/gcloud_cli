NAME
    gcloud alpha ml-engine jobs submit prediction - start a Cloud ML Engine
        batch prediction job

SYNOPSIS
    gcloud alpha ml-engine jobs submit prediction JOB --data-format=DATA_FORMAT
        --input-paths=INPUT_PATH,[INPUT_PATH,...] --output-path=OUTPUT_PATH
        --region=REGION (--model=MODEL | --model-dir=MODEL_DIR)
        [--batch-size=BATCH_SIZE] [--labels=[KEY=VALUE,...]]
        [--max-worker-count=MAX_WORKER_COUNT]
        [--runtime-version=RUNTIME_VERSION] [--version=VERSION]
        [--accelerator-count=ACCELERATOR_COUNT
          --accelerator-type=ACCELERATOR_TYPE] [GCLOUD_WIDE_FLAG ...]

DESCRIPTION
    (ALPHA) Start a Cloud ML Engine batch prediction job.

POSITIONAL ARGUMENTS
     JOB
        Name of the batch prediction job.

REQUIRED FLAGS
     --data-format=DATA_FORMAT
        Data format of the input files. DATA_FORMAT must be one of:

         text
            Text files with instances separated by the new-line character.
         tf-record
            TFRecord files; see
            https://www.tensorflow.org/versions/r0.12/how_tos/reading_data/index.html#file-formats
         tf-record-gzip
            GZIP-compressed TFRecord files.

     --input-paths=INPUT_PATH,[INPUT_PATH,...]
        Google Cloud Storage paths to the instances to run prediction on.

        Wildcards (*) accepted at the end of a path. More than one path can be
        specified if multiple file patterns are needed. For example,

            gs://my-bucket/instances*,gs://my-bucket/other-instances1

        will match any objects whose names start with instances in my-bucket as
        well as the other-instances1 bucket, while

            gs://my-bucket/instance-dir/*

        will match any objects in the instance-dir "directory" (since
        directories aren't a first-class Cloud Storage concept) of my-bucket.

     --output-path=OUTPUT_PATH
        Google Cloud Storage path to which to save the output. Example:
        gs://my-bucket/output.

     --region=REGION
        The Google Compute Engine region to run the job in.

     Exactly one of these must be specified:

       --model=MODEL
          Name of the model to use for prediction.

       --model-dir=MODEL_DIR
          Google Cloud Storage location where the model files are located.

OPTIONAL FLAGS
     --batch-size=BATCH_SIZE
        The number of records per batch. The service will buffer batch_size
        number of records in memory before invoking TensorFlow. Defaults to 64
        if not specified.

     --labels=[KEY=VALUE,...]
        A list of label KEY=VALUE pairs to add.

     --max-worker-count=MAX_WORKER_COUNT
        The maximum number of workers to be used for parallel processing.
        Defaults to 10 if not specified.

     --runtime-version=RUNTIME_VERSION
        The Google Cloud ML Engine runtime version for this job. Defaults to
        the latest stable version. See
        https://cloud.google.com/ml/docs/concepts/runtime-version-list for a
        list of accepted versions.

     --version=VERSION
        Model version to be used.

        This flag may only be given if --model is specified. If unspecified,
        the default version of the model will be used. To list versions for a
        model, run

            $ gcloud ml-engine versions list

     Accelerator Configuration.

       --accelerator-count=ACCELERATOR_COUNT
          The number of accelerators to attach to the machines. Must be >= 1.
          This flag must be specified if any of the other arguments in this
          group are specified.

       --accelerator-type=ACCELERATOR_TYPE
          The available types of accelerators. ACCELERATOR_TYPE must be one of:

       nvidia-tesla-k80
          NVIDIA Tesla K80 GPU
       nvidia-tesla-p100
          NVIDIA Tesla P100 GPU. + This flag must be specified if any of the
          other arguments in this group are specified.

GCLOUD WIDE FLAGS
    These flags are available to all commands: --account, --configuration,
    --flatten, --format, --help, --log-http, --project, --quiet, --trace-token,
    --user-output-enabled, --verbosity. Run $ gcloud help for details.

NOTES
    This command is currently in ALPHA and may change without notice. Usually,
    users of ALPHA commands and flags need to apply for access, agree to
    applicable terms, and have their projects whitelisted. Contact Google or
    sign up on a product's page for ALPHA access. Product pages can be found at
    https://cloud.google.com/products/. These variants are also available:

        $ gcloud ml-engine jobs submit prediction
        $ gcloud beta ml-engine jobs submit prediction

