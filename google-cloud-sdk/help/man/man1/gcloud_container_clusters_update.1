
.TH "GCLOUD_CONTAINER_CLUSTERS_UPDATE" 1



.SH "NAME"
.HP
gcloud container clusters update \- update cluster settings for an existing container cluster



.SH "SYNOPSIS"
.HP
\f5gcloud container clusters update\fR \fINAME\fR (\fB\-\-complete\-credential\-rotation\fR\ |\ \fB\-\-complete\-ip\-rotation\fR\ |\ \fB\-\-database\-encryption\-key\fR=\fIDATABASE_ENCRYPTION_KEY\fR\ |\ \fB\-\-disable\-database\-encryption\fR\ |\ \fB\-\-disable\-workload\-identity\fR\ |\ \fB\-\-enable\-autoscaling\fR\ |\ \fB\-\-enable\-binauthz\fR\ |\ \fB\-\-enable\-intra\-node\-visibility\fR\ |\ \fB\-\-enable\-legacy\-authorization\fR\ |\ \fB\-\-enable\-master\-authorized\-networks\fR\ |\ \fB\-\-enable\-network\-policy\fR\ |\ \fB\-\-enable\-shielded\-nodes\fR\ |\ \fB\-\-enable\-stackdriver\-kubernetes\fR\ |\ \fB\-\-enable\-vertical\-pod\-autoscaling\fR\ |\ \fB\-\-generate\-password\fR\ |\ \fB\-\-maintenance\-window\fR=\fISTART_TIME\fR\ |\ \fB\-\-node\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]\ |\ \fB\-\-release\-channel\fR=\fICHANNEL\fR\ |\ \fB\-\-remove\-labels\fR=[\fIKEY\fR,...]\ |\ \fB\-\-set\-password\fR\ |\ \fB\-\-start\-credential\-rotation\fR\ |\ \fB\-\-start\-ip\-rotation\fR\ |\ \fB\-\-update\-addons\fR=[\fIADDON\fR=\fIENABLED\fR|\fIDISABLED\fR,...]\ |\ \fB\-\-update\-labels\fR=[\fIKEY\fR=\fIVALUE\fR,...]\ |\ \fB\-\-workload\-pool\fR=\fIWORKLOAD_POOL\fR\ |\ \fB\-\-clear\-maintenance\-window\fR\ |\ \fB\-\-remove\-maintenance\-exclusion\fR=\fINAME\fR\ |\ [\fB\-\-add\-maintenance\-exclusion\-end\fR=\fITIME_STAMP\fR\ :\ \fB\-\-add\-maintenance\-exclusion\-name\fR=\fINAME\fR\ \fB\-\-add\-maintenance\-exclusion\-start\fR=\fITIME_STAMP\fR]\ |\ \fB\-\-maintenance\-window\-end\fR=\fITIME_STAMP\fR\ \fB\-\-maintenance\-window\-recurrence\fR=\fIRRULE\fR\ \fB\-\-maintenance\-window\-start\fR=\fITIME_STAMP\fR\ |\ \fB\-\-clear\-resource\-usage\-bigquery\-dataset\fR\ |\ \fB\-\-enable\-network\-egress\-metering\fR\ \fB\-\-enable\-resource\-consumption\-metering\fR\ \fB\-\-resource\-usage\-bigquery\-dataset\fR=\fIRESOURCE_USAGE_BIGQUERY_DATASET\fR\ |\ [\fB\-\-enable\-autoprovisioning\fR\ :\ \fB\-\-autoprovisioning\-config\-file\fR=\fIAUTOPROVISIONING_CONFIG_FILE\fR\ |\ \fB\-\-autoprovisioning\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]\ \fB\-\-max\-cpu\fR=\fIMAX_CPU\fR\ \fB\-\-max\-memory\fR=\fIMAX_MEMORY\fR\ \fB\-\-min\-cpu\fR=\fIMIN_CPU\fR\ \fB\-\-min\-memory\fR=\fIMIN_MEMORY\fR\ \fB\-\-autoprovisioning\-max\-surge\-upgrade\fR=\fIAUTOPROVISIONING_MAX_SURGE_UPGRADE\fR\ \fB\-\-autoprovisioning\-max\-unavailable\-upgrade\fR=\fIAUTOPROVISIONING_MAX_UNAVAILABLE_UPGRADE\fR\ \fB\-\-autoprovisioning\-scopes\fR=[\fISCOPE\fR,...]\ \fB\-\-autoprovisioning\-service\-account\fR=\fIAUTOPROVISIONING_SERVICE_ACCOUNT\fR\ \fB\-\-enable\-autoprovisioning\-autorepair\fR\ \fB\-\-enable\-autoprovisioning\-autoupgrade\fR\ [\fB\-\-max\-accelerator\fR=[\fItype\fR=\fITYPE\fR,\fIcount\fR=\fICOUNT\fR,...]\ :\ \fB\-\-min\-accelerator\fR=[\fItype\fR=\fITYPE\fR,\fIcount\fR=\fICOUNT\fR,...]]]\ |\ \fB\-\-logging\-service\fR=\fILOGGING_SERVICE\fR\ \fB\-\-monitoring\-service\fR=\fIMONITORING_SERVICE\fR\ |\ \fB\-\-password\fR=\fIPASSWORD\fR\ \fB\-\-enable\-basic\-auth\fR\ |\ \fB\-\-username\fR=\fIUSERNAME\fR,\ \fB\-u\fR\ \fIUSERNAME\fR) [\fB\-\-async\fR] [\fB\-\-master\-authorized\-networks\fR=\fINETWORK\fR,[\fINETWORK\fR,...]] [\fB\-\-node\-pool\fR=\fINODE_POOL\fR] [\fB\-\-max\-nodes\fR=\fIMAX_NODES\fR\ \fB\-\-min\-nodes\fR=\fIMIN_NODES\fR] [\fB\-\-region\fR=\fIREGION\fR\ |\ \fB\-\-zone\fR=\fIZONE\fR,\ \fB\-z\fR\ \fIZONE\fR] [\fIGCLOUD_WIDE_FLAG\ ...\fR]



.SH "DESCRIPTION"

Update cluster settings for an existing container cluster.



.SH "POSITIONAL ARGUMENTS"

.RS 2m
.TP 2m
\fINAME\fR
The name of the cluster to update.


.RE
.sp

.SH "REQUIRED FLAGS"

.RS 2m
.TP 2m

Exactly one of these must be specified:

.RS 2m
.TP 2m
\fB\-\-complete\-credential\-rotation\fR
Complete the IP and credential rotation for this cluster. For example:

.RS 2m
$ gcloud container clusters update example\-cluster \e
    \-\-complete\-credential\-rotation
.RE

This causes the cluster to stop serving its old IP, return to a single IP, and
invalidate old credentials.

.TP 2m
\fB\-\-complete\-ip\-rotation\fR
Complete the IP rotation for this cluster. For example:

.RS 2m
$ gcloud container clusters update example\-cluster \e
    \-\-complete\-ip\-rotation
.RE

This causes the cluster to stop serving its old IP, and return to a single IP
state.

.TP 2m
\fB\-\-database\-encryption\-key\fR=\fIDATABASE_ENCRYPTION_KEY\fR
Enable Database Encryption.

Enable database encryption that will be used to encrypt Kubernetes Secrets at
the application layer. The key provided should be the resource ID in the format
of
\f5projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]\fR.
For more information, see
https://cloud.google.com/kubernetes\-engine/docs/how\-to/encrypting\-secrets.

.TP 2m
\fB\-\-disable\-database\-encryption\fR
Disable database encryption.

Disable Database Encryption which encrypt Kubernetes Secrets at the application
layer. For more information, see
https://cloud.google.com/kubernetes\-engine/docs/how\-to/encrypting\-secrets.

.TP 2m
\fB\-\-disable\-workload\-identity\fR
Disable Workload Identity on the cluster.

For more information on Workload Identity, see

.RS 2m
https://cloud.google.com/kubernetes\-engine/docs/how\-to/workload\-identity
.RE

.TP 2m
\fB\-\-enable\-autoscaling\fR
Enables autoscaling for a node pool.

Enables autoscaling in the node pool specified by \-\-node\-pool or the default
node pool if \-\-node\-pool is not provided.

.TP 2m
\fB\-\-enable\-binauthz\fR
Enable Binary Authorization for this cluster.

.TP 2m
\fB\-\-enable\-intra\-node\-visibility\fR
Enable Intra\-node visibility for this cluster.

Enabling intra\-node visibility makes your intra\-node pod\-to\-pod traffic
visible to the networking fabric. With this feature, you can use VPC flow
logging or other VPC features for intra\-node traffic.

Enabling it on an existing cluster causes the cluster master and the cluster
nodes to restart, which might cause a disruption.

.TP 2m
\fB\-\-enable\-legacy\-authorization\fR
Enables the legacy ABAC authentication for the cluster. User rights are granted
through the use of policies which combine attributes together. For a detailed
look at these properties and related formats, see
https://kubernetes.io/docs/admin/authorization/abac/. To use RBAC permissions
instead, create or update your cluster with the option
\f5\-\-no\-enable\-legacy\-authorization\fR.

.TP 2m
\fB\-\-enable\-master\-authorized\-networks\fR
Allow only specified set of CIDR blocks (specified by the
\f5\-\-master\-authorized\-networks\fR flag) to connect to Kubernetes master
through HTTPS. Besides these blocks, the following have access as well:

.RS 2m
1) The private network the cluster connects to if
`\-\-enable\-private\-nodes` is specified.
2) Google Compute Engine Public IPs if `\-\-enable\-private\-nodes` is not
specified.
.RE

Use \f5\-\-no\-enable\-master\-authorized\-networks\fR to disable. When
disabled, public internet (0.0.0.0/0) is allowed to connect to Kubernetes master
through HTTPS.

.TP 2m
\fB\-\-enable\-network\-policy\fR
Enable network policy enforcement for this cluster. If you are enabling network
policy on an existing cluster the network policy addon must first be enabled on
the master by using \-\-update\-addons=NetworkPolicy=ENABLED flag.

.TP 2m
\fB\-\-enable\-shielded\-nodes\fR
Enable Shielded Nodes for this cluster. Enabling Shielded Nodes will enable a
more secure Node credential bootstrapping implementation. Starting with version
1.18, clusters will have shielded GKE nodes by default.

.TP 2m
\fB\-\-enable\-stackdriver\-kubernetes\fR
Enable Stackdriver Kubernetes monitoring and logging.

.TP 2m
\fB\-\-enable\-vertical\-pod\-autoscaling\fR
Enable vertical pod autoscaling for a cluster.

.TP 2m
\fB\-\-generate\-password\fR
Ask the server to generate a secure password and use that as the basic auth
password, keeping the existing username.

.TP 2m
\fB\-\-maintenance\-window\fR=\fISTART_TIME\fR
Set a time of day when you prefer maintenance to start on this cluster. For
example:

.RS 2m
$ gcloud container clusters update example\-cluster \e
    \-\-maintenance\-window=12:43
.RE

The time corresponds to the UTC time zone, and must be in HH:MM format.

Non\-emergency maintenance will occur in the 4 hour block starting at the
specified time.

This is mutually exclusive with the recurring maintenance windows and will
overwrite any existing window. Compatible with maintenance exclusions.

To remove an existing maintenance window from the cluster, use
\'\-\-clear\-maintenance\-window'.

.TP 2m
\fB\-\-node\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]
The set of zones in which the specified node footprint should be replicated. All
zones must be in the same region as the cluster's master(s), specified by the
\f5\-\-zone\fR or \f5\-\-region\fR flag. Additionally, for zonal clusters,
\f5\-\-node\-locations\fR must contain the cluster's primary zone. If not
specified, all nodes will be in the cluster's primary zone (for zonal clusters)
or spread across three randomly chosen zones within the cluster's region (for
regional clusters).

Note that \f5NUM_NODES\fR nodes will be created in each zone, such that if you
specify \f5\-\-num\-nodes=4\fR and choose two locations, 8 nodes will be
created.

Multiple locations can be specified, separated by commas. For example:

.RS 2m
$ gcloud container clusters update example\-cluster \e
    \-\-zone us\-central1\-a \e
    \-\-node\-locations us\-central1\-a,us\-central1\-b
.RE

.TP 2m
\fB\-\-release\-channel\fR=\fICHANNEL\fR
Subscribe or unsubscribe this cluster to a release channel.

When a cluster is subscribed to a release channel, Google maintains both the
master version and the node version. Node auto\-upgrade defaults to true and
cannot be disabled.

\fICHANNEL\fR must be one of:

.RS 2m
.TP 2m
\fBNone\fR
Use '\-\-release\-channel=None' to take a cluster off of a release channel.
Clusters on 'rapid' cannot be taken off of the release channel.

.TP 2m
\fBrapid\fR
\'rapid' channel is offered on an early access basis for customers who want to
test new releases.

WARNING: Versions available in the 'rapid' channel may be subject to unresolved
issues with no known workaround and are not subject to any SLAs.

.TP 2m
\fBregular\fR
Clusters subscribed to 'regular' receive versions that are considered GA
quality. 'regular' is intended for production users who want to take advantage
of new features.

.TP 2m
\fBstable\fR
Clusters subscribed to 'stable' receive versions that are known to be stable and
reliable in production.

.RE
.sp


.TP 2m
\fB\-\-remove\-labels\fR=[\fIKEY\fR,...]
Labels to remove from the Google Cloud resources in use by the Kubernetes Engine
cluster. These are unrelated to Kubernetes labels. Example:

.RS 2m
$ gcloud container clusters update example\-cluster \e
    \-\-remove\-labels=label_a,label_b
.RE

.TP 2m
\fB\-\-set\-password\fR
Set the basic auth password to the specified value, keeping the existing
username.

.TP 2m
\fB\-\-start\-credential\-rotation\fR
Start the rotation of IP and credentials for this cluster. For example:

.RS 2m
$ gcloud container clusters update example\-cluster \e
    \-\-start\-credential\-rotation
.RE

This causes the cluster to serve on two IPs, and will initiate a node upgrade to
point to the new IP.

.TP 2m
\fB\-\-start\-ip\-rotation\fR
Start the rotation of this cluster to a new IP. For example:

.RS 2m
$ gcloud container clusters update example\-cluster \e
    \-\-start\-ip\-rotation
.RE

This causes the cluster to serve on two IPs, and will initiate a node upgrade to
point to the new IP.

.TP 2m
\fB\-\-update\-addons\fR=[\fIADDON\fR=\fIENABLED\fR|\fIDISABLED\fR,...]
Cluster addons to enable or disable. Options are
HorizontalPodAutoscaling=ENABLED|DISABLED HttpLoadBalancing=ENABLED|DISABLED
KubernetesDashboard=ENABLED|DISABLED NetworkPolicy=ENABLED|DISABLED
CloudRun=ENABLED|DISABLED NodeLocalDNS=ENABLED|DISABLED

.TP 2m
\fB\-\-update\-labels\fR=[\fIKEY\fR=\fIVALUE\fR,...]
Labels to apply to the Google Cloud resources in use by the Kubernetes Engine
cluster. These are unrelated to Kubernetes labels. Example:

.RS 2m
$ gcloud container clusters update example\-cluster \e
    \-\-update\-labels=label_a=value1,label_b=value2
.RE

.TP 2m
\fB\-\-workload\-pool\fR=\fIWORKLOAD_POOL\fR
Enable Workload Identity on the cluster.

When enabled, Kubernetes service accounts will be able to act as Cloud IAM
Service Accounts, through the provided workload pool.

Currently, the only accepted workload pool is the workload pool of the Cloud
project containing the cluster, \f5PROJECT_ID.svc.id.goog\fR.

For more information on Workload Identity, see

.RS 2m
https://cloud.google.com/kubernetes\-engine/docs/how\-to/workload\-identity
.RE

.TP 2m

At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-clear\-maintenance\-window\fR
If set, remove the maintenance window that was set with \-\-maintenance\-window
family of flags.

.TP 2m
\fB\-\-remove\-maintenance\-exclusion\fR=\fINAME\fR
Name of a maintenance exclusion to remove. If you hadn't specified a name, one
was auto\-generated. Get it with $ gcloud container clusters describe.

.TP 2m

Sets a period of time in which maintenance should not occur. This is compatible
with both daily and recurring maintenance windows.

Example:

.RS 2m
$ gcloud container clusters update example\-cluster   \e
    \-\-add\-maintenance\-exclusion\-name=holidays\-2000   \e
    \-\-add\-maintenance\-exclusion\-start=2000\-11\-20T00:00:00   \e
    \-\-add\-maintenance\-exclusion\-end=2000\-12\-31T23:59:59
.RE



.RS 2m
.TP 2m
\fB\-\-add\-maintenance\-exclusion\-end\fR=\fITIME_STAMP\fR
End time of the exclusion window. Must take place after the start time. See $
gcloud topic datetimes for information on time formats. This flag must be
specified if any of the other arguments in this group are specified.

.TP 2m
\fB\-\-add\-maintenance\-exclusion\-name\fR=\fINAME\fR
A descriptor for the exclusion that can be used to remove it. If not specified,
it will be autogenerated.

.TP 2m
\fB\-\-add\-maintenance\-exclusion\-start\fR=\fITIME_STAMP\fR
Start time of the exclusion window (can occur in the past). If not specified,
the current time will be used. See $ gcloud topic datetimes for information on
time formats.

.RE
.sp
.TP 2m

Set a flexible maintenance window by specifying a window that recurs per an RFC
5545 RRULE. Non\-emergency maintenance will occur in the recurring windows.

Examples:

For a 9\-5 Mon\-Wed UTC\-4 maintenance window:

.RS 2m
$ gcloud container clusters update example\-cluster   \e
    \-\-maintenance\-window\-start=2000\-01\-01T09:00:00\-04:00   \e
    \-\-maintenance\-window\-end=2000\-01\-01T17:00:00\-04:00   \e
    \-\-maintenance\-window\-recurrence='FREQ=WEEKLY;BYDAY=MO,TU,WE'
.RE

For a daily window from 22:00 \- 04:00 UTC:

.RS 2m
$ gcloud container clusters update example\-cluster   \e
    \-\-maintenance\-window\-start=2000\-01\-01T22:00:00Z   \e
    \-\-maintenance\-window\-end=2000\-01\-02T04:00:00Z   \e
    \-\-maintenance\-window\-recurrence=FREQ=DAILY
.RE



.RS 2m
.TP 2m
\fB\-\-maintenance\-window\-end\fR=\fITIME_STAMP\fR
End time of the first window (can occur in the past). Must take place after the
start time. The difference in start and end time specifies the length of each
recurrence. See $ gcloud topic datetimes for information on time formats. This
flag must be specified if any of the other arguments in this group are
specified.

.TP 2m
\fB\-\-maintenance\-window\-recurrence\fR=\fIRRULE\fR
An RFC 5545 RRULE, specifying how the window will recur. Note that minimum
requirements for maintenance periods will be enforced. Note that FREQ=SECONDLY,
MINUTELY, and HOURLY are not supported. This flag must be specified if any of
the other arguments in this group are specified.

.TP 2m
\fB\-\-maintenance\-window\-start\fR=\fITIME_STAMP\fR
Start time of the first window (can occur in the past). The start time
influences when the window will start for recurrences. See $ gcloud topic
datetimes for information on time formats. This flag must be specified if any of
the other arguments in this group are specified.

.RE
.RE
.sp
.TP 2m

Exports cluster's usage of cloud resources At most one of these may be
specified:

.RS 2m
.TP 2m
\fB\-\-clear\-resource\-usage\-bigquery\-dataset\fR
Disables exporting cluster resource usage to BigQuery.

.TP 2m
\fB\-\-enable\-network\-egress\-metering\fR
Enable network egress metering on this cluster.

When enabled, a DaemonSet is deployed into the cluster. Each DaemonSet pod
meters network egress traffic by collecting data from the conntrack table, and
exports the metered metrics to the specified destination.

Network egress metering is disabled if this flag is omitted, or when
\f5\-\-no\-enable\-network\-egress\-metering\fR is set.

.TP 2m
\fB\-\-enable\-resource\-consumption\-metering\fR
Enable resource consumption metering on this cluster.

When enabled, a table will be created in the specified BigQuery dataset to store
resource consumption data. The resulting table can be joined with the resource
usage table or with BigQuery billing export.

To disable resource consumption metering, set
\f5\-\-no\-enable\-resource\-consumption\- metering\fR. If this flag is omitted,
then resource consumption metering will remain enabled or disabled depending on
what is already configured for this cluster.

.TP 2m
\fB\-\-resource\-usage\-bigquery\-dataset\fR=\fIRESOURCE_USAGE_BIGQUERY_DATASET\fR
The name of the BigQuery dataset to which the cluster's usage of cloud resources
is exported. A table will be created in the specified dataset to store cluster
resource usage. The resulting table can be joined with BigQuery Billing Export
to produce a fine\-grained cost breakdown.

Example:

.RS 2m
$ gcloud container clusters update example\-cluster \e
    \-\-resource\-usage\-bigquery\-dataset=example_bigquery_dataset_name
.RE

.RE
.sp
.TP 2m

Node autoprovisioning

.RS 2m
.TP 2m
\fB\-\-enable\-autoprovisioning\fR
Enables node autoprovisioning for a cluster.

Cluster Autoscaler will be able to create new node pools. Requires maximum CPU
and memory limits to be specified. This flag must be specified if any of the
other arguments in this group are specified.

.TP 2m

At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-autoprovisioning\-config\-file\fR=\fIAUTOPROVISIONING_CONFIG_FILE\fR
Path of the JSON/YAML file which contains information about the cluster's node
autoprovisioning configuration. Currently it contains a list of resource limits,
identity defaults for autoprovisioning, node upgrade settings, node management
settings, minimum cpu platform, and node locations for autoprovisioning.

Resource limits are specified in the field 'resourceLimits'. Each resource
limits definition contains three fields: resourceType, maximum and minimum.
Resource type can be "cpu", "memory" or an accelerator (e.g.
"nvidia\-tesla\-k80" for nVidia Tesla K80). Use gcloud compute
accelerator\-types list to learn about available accelerator types. Maximum is
the maximum allowed amount with the unit of the resource. Minimum is the minimum
allowed amount with the unit of the resource.

Identity default contains at most one of the below fields: serviceAccount: The
Google Cloud Platform Service Account to be used by node VMs in autoprovisioned
node pools. If not specified, the project's default service account is used.
scopes: A list of scopes to be used by node instances in autoprovisioned node
pools. Multiple scopes can be specified, separated by commas. For information on
defaults, look at:
https://cloud.google.com/sdk/gcloud/reference/container/clusters/create#\-\-scopes

Node Upgrade settings are specified under the field 'upgradeSettings', which has
the following fields: maxSurgeUpgrade: Number of extra (surge) nodes to be
created on each upgrade of an autoprovisioned node pool. maxUnavailableUpgrade:
Number of nodes that can be unavailable at the same time on each upgrade of an
autoprovisioned node pool.

Node Management settings are specified under the field 'nodeManagement', which
has the following fields: enableAutoUpgrade: A boolean field that indicates if
node autoupgrade is enabled for autoprovisioned node pools. enableAutoRepair: A
boolean field that indicates if node autorepair is enabled for autoprovisioned
node pools.

minCpuPlatform: If specified, new autoprovisioned nodes will be scheduled on
host with specified CPU architecture or a newer one. Note: Min CPU platform can
only be specified in Beta and Alpha.

Autoprovisioning locations is a set of zones where new node pools can be created
by Autoprovisioning. Autoprovisioning locations are specified in the field
\'autoprovisioningLocations'. All zones must be in the same region as the
cluster's master(s).

.TP 2m

Flags to configure autoprovisioned nodes

.RS 2m
.TP 2m
\fB\-\-autoprovisioning\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]
Set of zones where new node pools can be created by autoprovisioning. All zones
must be in the same region as the cluster's master(s). Multiple locations can be
specified, separated by commas.

.TP 2m
\fB\-\-max\-cpu\fR=\fIMAX_CPU\fR
Maximum number of cores in the cluster.

Maximum number of cores to which the cluster can scale.

.TP 2m
\fB\-\-max\-memory\fR=\fIMAX_MEMORY\fR
Maximum memory in the cluster.

Maximum number of gigabytes of memory to which the cluster can scale.

.TP 2m
\fB\-\-min\-cpu\fR=\fIMIN_CPU\fR
Minimum number of cores in the cluster.

Minimum number of cores to which the cluster can scale.

.TP 2m
\fB\-\-min\-memory\fR=\fIMIN_MEMORY\fR
Minimum memory in the cluster.

Minimum number of gigabytes of memory to which the cluster can scale.

.TP 2m

Flags to specify upgrade settings for autoprovisioned nodes:

.RS 2m
.TP 2m
\fB\-\-autoprovisioning\-max\-surge\-upgrade\fR=\fIAUTOPROVISIONING_MAX_SURGE_UPGRADE\fR
Number of extra (surge) nodes to be created on each upgrade of an
autoprovisioned node pool. This flag must be specified if any of the other
arguments in this group are specified.

.TP 2m
\fB\-\-autoprovisioning\-max\-unavailable\-upgrade\fR=\fIAUTOPROVISIONING_MAX_UNAVAILABLE_UPGRADE\fR
Number of nodes that can be unavailable at the same time on each upgrade of an
autoprovisioned node pool. This flag must be specified if any of the other
arguments in this group are specified.

.RE
.sp
.TP 2m

Flags to specify identity for autoprovisioned nodes:

.RS 2m
.TP 2m
\fB\-\-autoprovisioning\-scopes\fR=[\fISCOPE\fR,...]
The scopes be used by node instances in autoprovisioned node pools. Multiple
scopes can be specified, separated by commas. For information on defaults, look
at:
https://cloud.google.com/sdk/gcloud/reference/container/clusters/create#\-\-scopes

.TP 2m
\fB\-\-autoprovisioning\-service\-account\fR=\fIAUTOPROVISIONING_SERVICE_ACCOUNT\fR
The Google Cloud Platform Service Account to be used by node VMs in
autoprovisioned node pools. If not specified, the project default service
account is used.

.RE
.sp
.TP 2m

Flags to specify node management settings for autoprovisioned nodes:

.RS 2m
.TP 2m
\fB\-\-enable\-autoprovisioning\-autorepair\fR
Enable node autorepair for autoprovisioned node pools. Use
\-\-no\-enable\-autoprovisioning\-autorepair to disable. This flag must be
specified if any of the other arguments in this group are specified.

.TP 2m
\fB\-\-enable\-autoprovisioning\-autoupgrade\fR
Enable node autoupgrade for autoprovisioned node pools. Use
\-\-no\-enable\-autoprovisioning\-autoupgrade to disable. This flag must be
specified if any of the other arguments in this group are specified.

.RE
.sp
.TP 2m

Arguments to set limits on accelerators:

.RS 2m
.TP 2m
\fB\-\-max\-accelerator\fR=[\fItype\fR=\fITYPE\fR,\fIcount\fR=\fICOUNT\fR,...]
Sets maximum limit for a single type of accelerators (e.g. GPUs) in cluster.

.RE
.RE
.RE
.RE
.sp
.TP 2m
\fBtype\fR
(Required) The specific type (e.g. nvidia\-tesla\-k80 for nVidia Tesla K80) of
accelerator for which the limit is set. Use \f5gcloud compute accelerator\-types
list\fR to learn about all available accelerator types.

.TP 2m
\fBcount\fR
(Required) The maximum number of accelerators to which the cluster can be
scaled. This flag must be specified if any of the other arguments in this group
are specified.

.RS 2m
.TP 2m
\fB\-\-min\-accelerator\fR=[\fItype\fR=\fITYPE\fR,\fIcount\fR=\fICOUNT\fR,...]
Sets minimum limit for a single type of accelerators (e.g. GPUs) in cluster.
Defaults to 0 for all accelerator types if it isn't set.

.RE
.sp
.TP 2m
\fBtype\fR
(Required) The specific type (e.g. nvidia\-tesla\-k80 for nVidia Tesla K80) of
accelerator for which the limit is set. Use \f5gcloud compute accelerator\-types
list\fR to learn about all available accelerator types.

.TP 2m
\fBcount\fR
(Required) The minimum number of accelerators to which the cluster can be
scaled.

.TP 2m
\fB\-\-logging\-service\fR=\fILOGGING_SERVICE\fR
Logging service to use for the cluster. Options are:
"logging.googleapis.com/kubernetes" (the Google Cloud Logging service with
Kubernetes\-native resource model enabled), "logging.googleapis.com" (the Google
Cloud Logging service), "none" (logs will not be exported from the cluster)

.TP 2m
\fB\-\-monitoring\-service\fR=\fIMONITORING_SERVICE\fR
Monitoring service to use for the cluster. Options are:
"monitoring.googleapis.com/kubernetes" (the Google Cloud Monitoring service with
Kubernetes\-native resource model enabled), "monitoring.googleapis.com" (the
Google Cloud Monitoring service), "none" (no metrics will be exported from the
cluster)

.TP 2m

Basic auth

.RS 2m
.TP 2m
\fB\-\-password\fR=\fIPASSWORD\fR
The password to use for cluster auth. Defaults to a server\-specified
randomly\-generated string.

.TP 2m

Options to specify the username. At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-enable\-basic\-auth\fR
Enable basic (username/password) auth for the cluster.
\f5\-\-enable\-basic\-auth\fR is an alias for \f5\-\-username=admin\fR;
\f5\-\-no\-enable\-basic\-auth\fR is an alias for \f5\-\-username=""\fR. Use
\f5\-\-password\fR to specify a password; if not, the server will randomly
generate one. For cluster versions before 1.12, if neither
\f5\-\-enable\-basic\-auth\fR nor \f5\-\-username\fR is specified,
\f5\-\-enable\-basic\-auth\fR will default to \f5true\fR. After 1.12,
\f5\-\-enable\-basic\-auth\fR will default to \f5false\fR.

.TP 2m
\fB\-\-username\fR=\fIUSERNAME\fR, \fB\-u\fR \fIUSERNAME\fR
The user name to use for basic auth for the cluster. Use \f5\-\-password\fR to
specify a password; if not, the server will randomly generate one.


.RE
.RE
.RE
.RE
.sp

.SH "OPTIONAL FLAGS"

.RS 2m
.TP 2m
\fB\-\-async\fR
Return immediately, without waiting for the operation in progress to complete.

.TP 2m
\fB\-\-master\-authorized\-networks\fR=\fINETWORK\fR,[\fINETWORK\fR,...]
The list of CIDR blocks (up to 100 for private cluster, 50 for public cluster)
that are allowed to connect to Kubernetes master through HTTPS. Specified in
CIDR notation (e.g. 1.2.3.4/30). Cannot be specified unless
\f5\-\-enable\-master\-authorized\-networks\fR is also specified.

.TP 2m
\fB\-\-node\-pool\fR=\fINODE_POOL\fR
Node pool to be updated.

.TP 2m

Cluster autoscaling

.RS 2m
.TP 2m
\fB\-\-max\-nodes\fR=\fIMAX_NODES\fR
Maximum number of nodes in the node pool.

Maximum number of nodes to which the node pool specified by \-\-node\-pool (or
default node pool if unspecified) can scale. Ignored unless
\-\-enable\-autoscaling is also specified.

.TP 2m
\fB\-\-min\-nodes\fR=\fIMIN_NODES\fR
Minimum number of nodes in the node pool.

Minimum number of nodes to which the node pool specified by \-\-node\-pool (or
default node pool if unspecified) can scale. Ignored unless
\-\-enable\-autoscaling is also specified.

.RE
.sp
.TP 2m

At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-region\fR=\fIREGION\fR
Compute region (e.g. us\-central1) for the cluster.

.TP 2m
\fB\-\-zone\fR=\fIZONE\fR, \fB\-z\fR \fIZONE\fR
Compute zone (e.g. us\-central1\-a) for the cluster. Overrides the default
\fBcompute/zone\fR property value for this command invocation.


.RE
.RE
.sp

.SH "GCLOUD WIDE FLAGS"

These flags are available to all commands: \-\-account, \-\-billing\-project,
\-\-configuration, \-\-flags\-file, \-\-flatten, \-\-format, \-\-help,
\-\-impersonate\-service\-account, \-\-log\-http, \-\-project, \-\-quiet,
\-\-trace\-token, \-\-user\-output\-enabled, \-\-verbosity.

Run \fB$ gcloud help\fR for details.



.SH "EXAMPLES"

To enable autoscaling for an existing cluster, run:

.RS 2m
$ gcloud container clusters update sample\-cluster \e
    \-\-enable\-autoscaling
.RE



.SH "NOTES"

These variants are also available:

.RS 2m
$ gcloud alpha container clusters update
$ gcloud beta container clusters update
.RE

