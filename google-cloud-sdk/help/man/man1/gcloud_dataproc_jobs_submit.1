
.TH "GCLOUD_DATAPROC_JOBS_SUBMIT" 1



.SH "NAME"
.HP
gcloud dataproc jobs submit \- submit Google Cloud Dataproc jobs to execute on a cluster



.SH "SYNOPSIS"
.HP
\f5gcloud dataproc jobs submit\fR \fICOMMAND\fR [\fB\-\-async\fR] [\fB\-\-bucket\fR=\fIBUCKET\fR] [\fB\-\-region\fR=\fIREGION\fR] [\fIGCLOUD_WIDE_FLAG\ ...\fR]



.SH "DESCRIPTION"

Submit Google Cloud Dataproc jobs to execute on a cluster.



.SH "FLAGS"

.RS 2m
.TP 2m
\fB\-\-async\fR
Return immediately, without waiting for the operation in progress to complete.

.TP 2m
\fB\-\-bucket\fR=\fIBUCKET\fR
The Cloud Storage bucket to stage files in. Defaults to the cluster's configured
bucket.

.TP 2m
\fB\-\-region\fR=\fIREGION\fR
Cloud Dataproc region to use. Each Cloud Dataproc region constitutes an
independent resource namespace constrained to deploying instances into Compute
Engine zones inside the region. Overrides the default \fBdataproc/region\fR
property value for this command invocation.


.RE
.sp

.SH "GCLOUD WIDE FLAGS"

These flags are available to all commands: \-\-account, \-\-billing\-project,
\-\-configuration, \-\-flags\-file, \-\-flatten, \-\-format, \-\-help,
\-\-impersonate\-service\-account, \-\-log\-http, \-\-project, \-\-quiet,
\-\-trace\-token, \-\-user\-output\-enabled, \-\-verbosity.

Run \fB$ gcloud help\fR for details.



.SH "COMMANDS"

\f5\fICOMMAND\fR\fR is one of the following:

.RS 2m
.TP 2m
\fBhadoop\fR
Submit a Hadoop job to a cluster.

.TP 2m
\fBhive\fR
Submit a Hive job to a cluster.

.TP 2m
\fBpig\fR
Submit a Pig job to a cluster.

.TP 2m
\fBpyspark\fR
Submit a PySpark job to a cluster.

.TP 2m
\fBspark\fR
Submit a Spark job to a cluster.

.TP 2m
\fBspark\-r\fR
Submit a SparkR job to a cluster.

.TP 2m
\fBspark\-sql\fR
Submit a Spark SQL job to a cluster.


.RE
.sp

.SH "EXAMPLES"

To submit a Hadoop MapReduce job, run:

.RS 2m
$ gcloud dataproc jobs submit hadoop \-\-cluster my_cluster \e
    \-\-jar my_jar.jar \-\- arg1 arg2
.RE

To submit a Spark Scala or Java job, run:

.RS 2m
$ gcloud dataproc jobs submit spark \-\-cluster my_cluster \e
    \-\-jar my_jar.jar \-\- arg1 arg2
.RE

To submit a PySpark job, run:

.RS 2m
$ gcloud dataproc jobs submit pyspark \-\-cluster my_cluster \e
    my_script.py \-\- arg1 arg2
.RE

To submit a Spark SQL job, run:

.RS 2m
$ gcloud dataproc jobs submit spark\-sql \-\-cluster my_cluster \e
    \-\-file my_queries.q
.RE

To submit a Pig job, run:

.RS 2m
$ gcloud dataproc jobs submit pig \-\-cluster my_cluster \e
    \-\-file my_script.pig
.RE

To submit a Hive job, run:

.RS 2m
$ gcloud dataproc jobs submit hive \-\-cluster my_cluster \e
    \-\-file my_queries.q
.RE



.SH "NOTES"

These variants are also available:

.RS 2m
$ gcloud alpha dataproc jobs submit
$ gcloud beta dataproc jobs submit
.RE

