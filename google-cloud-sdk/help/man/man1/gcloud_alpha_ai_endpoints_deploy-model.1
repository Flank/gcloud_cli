
.TH "GCLOUD_ALPHA_AI_ENDPOINTS_DEPLOY\-MODEL" 1



.SH "NAME"
.HP
gcloud alpha ai endpoints deploy\-model \- deploy a model to an existing AI Platform endpoint



.SH "SYNOPSIS"
.HP
\f5gcloud alpha ai endpoints deploy\-model\fR (\fIENDPOINT\fR\ :\ \fB\-\-region\fR=\fIREGION\fR) \fB\-\-display\-name\fR=\fIDISPLAY_NAME\fR \fB\-\-min\-replica\-count\fR=\fIMIN_REPLICA_COUNT\fR \fB\-\-model\fR=\fIMODEL\fR [\fB\-\-accelerator\fR=[\fIcount\fR=\fICOUNT\fR],[\fItype\fR=\fITYPE\fR]] [\fB\-\-machine\-type\fR=\fIMACHINE_TYPE\fR] [\fB\-\-max\-replica\-count\fR=\fIMAX_REPLICA_COUNT\fR] [\fB\-\-traffic\-split\fR=[\fIDEPLOYED_MODEL_ID\fR=\fIVALUE\fR,...]] [\fIGCLOUD_WIDE_FLAG\ ...\fR]



.SH "DESCRIPTION"

\fB(ALPHA)\fR Deploy a model to an existing AI Platform endpoint.



.SH "POSITIONAL ARGUMENTS"

.RS 2m
.TP 2m

Endpoint resource \- The endpoint to deploy a model to. The arguments in this
group can be used to specify the attributes of this resource. (NOTE) Some
attributes are not given arguments in this group but can be set in other ways.
To set the [project] attribute: provide the argument [endpoint] on the command
line with a fully specified name; provide the argument [\-\-project] on the
command line; set the property [core/project]. This must be specified.

.RS 2m
.TP 2m
\fIENDPOINT\fR
ID of the endpoint or fully qualified identifier for the endpoint. This
positional must be specified if any of the other arguments in this group are
specified.

.TP 2m
\fB\-\-region\fR=\fIREGION\fR
Cloud region for the endpoint.


.RE
.RE
.sp

.SH "REQUIRED FLAGS"

.RS 2m
.TP 2m
\fB\-\-display\-name\fR=\fIDISPLAY_NAME\fR
Display name of the deployed model.

.TP 2m
\fB\-\-min\-replica\-count\fR=\fIMIN_REPLICA_COUNT\fR
Minimum number of machine replicas the deployed model will be always deployed
on.

.TP 2m
\fB\-\-model\fR=\fIMODEL\fR
Id of the uploaded model.


.RE
.sp

.SH "OPTIONAL FLAGS"

.RS 2m
.TP 2m
\fB\-\-accelerator\fR=[\fIcount\fR=\fICOUNT\fR],[\fItype\fR=\fITYPE\fR]
Manage the accelerator config for GPU serving. When deploying a model with
Compute Engine Machine Types, a GPU accelerator may also be selected.

.RS 2m
.TP 2m
\fBtype\fR
The type of the accelerator. Choices are 'nvidia\-tesla\-k80',
\'nvidia\-tesla\-p100', 'nvidia\-tesla\-p4', 'nvidia\-tesla\-t4',
\'nvidia\-tesla\-v100'.

.TP 2m
\fBcount\fR
The number of accelerators to attach to each machine running the job. This is
usually 1. If not specified, the default value is 1.

For example: \f5\-\-accelerator=type=nvidia\-tesla\-k80,count=1\fR

.RE
.sp
.TP 2m
\fB\-\-machine\-type\fR=\fIMACHINE_TYPE\fR
Type of machine on which to serve the model. Currently only applies to online
prediction. If the uploaded models use dedicated resources, the machine type is
a required field for deployment.

.TP 2m
\fB\-\-max\-replica\-count\fR=\fIMAX_REPLICA_COUNT\fR
Maximum number of machine replicas the deployed model will be always deployed
on.

.TP 2m
\fB\-\-traffic\-split\fR=[\fIDEPLOYED_MODEL_ID\fR=\fIVALUE\fR,...]
List of paris of deployed model id and value to set as traffic split.


.RE
.sp

.SH "GCLOUD WIDE FLAGS"

These flags are available to all commands: \-\-account, \-\-billing\-project,
\-\-configuration, \-\-flags\-file, \-\-flatten, \-\-format, \-\-help,
\-\-impersonate\-service\-account, \-\-log\-http, \-\-project, \-\-quiet,
\-\-trace\-token, \-\-user\-output\-enabled, \-\-verbosity.

Run \fB$ gcloud help\fR for details.



.SH "NOTES"

This command is currently in ALPHA and may change without notice. If this
command fails with API permission errors despite specifying the right project,
you may be trying to access an API with an invitation\-only early access
allowlist. This variant is also available:

.RS 2m
$ gcloud beta ai endpoints deploy\-model
.RE

