
.TH "GCLOUD_ALPHA_CONTAINER_NODE\-POOLS_CREATE" 1



.SH "NAME"
.HP
gcloud alpha container node\-pools create \- create a node pool in a running cluster



.SH "SYNOPSIS"
.HP
\f5gcloud alpha container node\-pools create\fR \fINAME\fR [\fB\-\-accelerator\fR=[\fItype\fR=\fITYPE\fR,[\fIcount\fR=\fICOUNT\fR],...]] [\fB\-\-boot\-disk\-kms\-key\fR=\fIBOOT_DISK_KMS_KEY\fR] [\fB\-\-cluster\fR=\fICLUSTER\fR] [\fB\-\-disk\-size\fR=\fIDISK_SIZE\fR] [\fB\-\-disk\-type\fR=\fIDISK_TYPE\fR] [\fB\-\-enable\-autoprovisioning\fR] [\fB\-\-enable\-autorepair\fR] [\fB\-\-no\-enable\-autoupgrade\fR] [\fB\-\-image\-type\fR=\fIIMAGE_TYPE\fR] [\fB\-\-linux\-sysctls\fR=\fIKEY\fR=\fIVALUE\fR,[\fIKEY\fR=\fIVALUE\fR,...]] [\fB\-\-machine\-type\fR=\fIMACHINE_TYPE\fR,\ \fB\-m\fR\ \fIMACHINE_TYPE\fR] [\fB\-\-max\-pods\-per\-node\fR=\fIMAX_PODS_PER_NODE\fR] [\fB\-\-max\-surge\-upgrade\fR=\fIMAX_SURGE_UPGRADE\fR;\ default=1] [\fB\-\-max\-unavailable\-upgrade\fR=\fIMAX_UNAVAILABLE_UPGRADE\fR] [\fB\-\-metadata\fR=\fIKEY\fR=\fIVALUE\fR,[\fIKEY\fR=\fIVALUE\fR,...]] [\fB\-\-metadata\-from\-file\fR=\fIKEY\fR=\fILOCAL_FILE_PATH\fR,[...]] [\fB\-\-min\-cpu\-platform\fR=\fIPLATFORM\fR] [\fB\-\-node\-group\fR=\fINODE_GROUP\fR] [\fB\-\-node\-labels\fR=[\fINODE_LABEL\fR,...]] [\fB\-\-node\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]] [\fB\-\-node\-taints\fR=[\fINODE_TAINT\fR,...]] [\fB\-\-node\-version\fR=\fINODE_VERSION\fR] [\fB\-\-num\-nodes\fR=\fINUM_NODES\fR;\ default=3] [\fB\-\-preemptible\fR] [\fB\-\-sandbox\fR=[\fItype\fR=\fITYPE\fR]] [\fB\-\-shielded\-integrity\-monitoring\fR] [\fB\-\-shielded\-secure\-boot\fR] [\fB\-\-system\-config\-from\-file\fR=\fISYSTEM_CONFIG_FROM_FILE\fR] [\fB\-\-tags\fR=\fITAG\fR,[\fITAG\fR,...]] [\fB\-\-workload\-metadata\fR=\fIWORKLOAD_METADATA\fR] [\fB\-\-enable\-autoscaling\fR\ \fB\-\-max\-nodes\fR=\fIMAX_NODES\fR\ \fB\-\-min\-nodes\fR=\fIMIN_NODES\fR] [\fB\-\-local\-ssd\-count\fR=\fILOCAL_SSD_COUNT\fR\ |\ \fB\-\-local\-ssd\-volumes\fR=[[\fIcount\fR=\fICOUNT\fR],[\fItype\fR=\fITYPE\fR],[\fIformat\fR=\fIFORMAT\fR],...]] [\fB\-\-region\fR=\fIREGION\fR\ |\ \fB\-\-zone\fR=\fIZONE\fR,\ \fB\-z\fR\ \fIZONE\fR] [\fB\-\-reservation\fR=\fIRESERVATION\fR\ \fB\-\-reservation\-affinity\fR=\fIRESERVATION_AFFINITY\fR] [\fB\-\-scopes\fR=[\fISCOPE\fR,...];\ default="gke\-default"\ \fB\-\-service\-account\fR=\fISERVICE_ACCOUNT\fR] [\fIGCLOUD_WIDE_FLAG\ ...\fR]



.SH "DESCRIPTION"

\fB(ALPHA)\fR \fBgcloud alpha container node\-pools create\fR facilitates the
creation of a node pool in a Google Kubernetes Engine cluster. A variety of
options exists to customize the node configuration and the number of nodes
created.



.SH "POSITIONAL ARGUMENTS"

.RS 2m
.TP 2m
\fINAME\fR
The name of the node pool to create.


.RE
.sp

.SH "FLAGS"

.RS 2m
.TP 2m
\fB\-\-accelerator\fR=[\fItype\fR=\fITYPE\fR,[\fIcount\fR=\fICOUNT\fR],...]
Attaches accelerators (e.g. GPUs) to all nodes.

.RS 2m
.TP 2m
\fBtype\fR
(Required) The specific type (e.g. nvidia\-tesla\-k80 for nVidia Tesla K80) of
accelerator to attach to the instances. Use \f5gcloud compute accelerator\-types
list\fR to learn about all available accelerator types.

.TP 2m
\fBcount\fR
(Optional) The number of accelerators to attach to the instances. The default
value is 1.

.RE
.sp
.TP 2m
\fB\-\-boot\-disk\-kms\-key\fR=\fIBOOT_DISK_KMS_KEY\fR
The Customer Managed Encryption Key used to encrypt the boot disk attached to
each node in the node pool. This should be of the form
projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME].
For more information about protecting resources with Cloud KMS Keys please see:
https://cloud.google.com/compute/docs/disks/customer\-managed\-encryption

.TP 2m
\fB\-\-cluster\fR=\fICLUSTER\fR
The cluster to add the node pool to. Overrides the default
\fBcontainer/cluster\fR property value for this command invocation.

.TP 2m
\fB\-\-disk\-size\fR=\fIDISK_SIZE\fR
Size for node VM boot disks. Defaults to 100GB.

.TP 2m
\fB\-\-disk\-type\fR=\fIDISK_TYPE\fR
Type of the node VM boot disk. Defaults to pd\-standard. \fIDISK_TYPE\fR must be
one of: \fBpd\-standard\fR, \fBpd\-ssd\fR.

.TP 2m
\fB\-\-enable\-autoprovisioning\fR
Enables Cluster Autoscaler to treat the node pool as if it was autoprovisioned.

Cluster Autoscaler will be able to delete the node pool if it's unneeded.

.TP 2m
\fB\-\-enable\-autorepair\fR
Enable node autorepair feature for a node pool.

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \-\-enable\-autorepair
.RE

Node autorepair is enabled by default for node pools using COS or COS_CONTAINERD
as a base image, use \-\-no\-enable\-autorepair to disable.

See https://cloud.google.com/kubernetes\-engine/docs/how\-to/node\-auto\-repair
for more info.

.TP 2m
\fB\-\-enable\-autoupgrade\fR
Sets autoupgrade feature for a node pool.

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \-\-enable\-autoupgrade
.RE

See https://cloud.google.com/kubernetes\-engine/docs/node\-auto\-upgrades for
more info.

Enabled by default, use \fB\-\-no\-enable\-autoupgrade\fR to disable.

.TP 2m
\fB\-\-image\-type\fR=\fIIMAGE_TYPE\fR
The image type to use for the node pool. Defaults to server\-specified.

Image Type specifies the base OS that the nodes in the node pool will run on. If
an image type is specified, that will be assigned to the node pool and all
future upgrades will use the specified image type. If it is not specified the
server will pick the default image type.

The default image type and the list of valid image types are available using the
following command.

.RS 2m
$ gcloud container get\-server\-config
.RE

.TP 2m
\fB\-\-linux\-sysctls\fR=\fIKEY\fR=\fIVALUE\fR,[\fIKEY\fR=\fIVALUE\fR,...]
Linux kernel parameters to be applied to all nodes in the new node pool as well
as the pods running on the nodes.

Example:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-linux\-sysctls="net.core.somaxconn=1024,net.ipv4.tcp_rmem=4096\e
 87380 6291456"
.RE

.TP 2m
\fB\-\-machine\-type\fR=\fIMACHINE_TYPE\fR, \fB\-m\fR \fIMACHINE_TYPE\fR
The type of machine to use for nodes. Defaults to n1\-standard\-1. The list of
predefined machine types is available using the following command:

.RS 2m
$ gcloud compute machine\-types list
.RE

You can also specify custom machine types with the string "custom\-CPUS\-RAM"
where \f5CPUS\fR is the number of virtual CPUs and \f5RAM\fR is the amount of
RAM in MiB.

For example, to create a node pool using custom machines with 2 vCPUs and 12 GB
of RAM:

.RS 2m
$ gcloud alpha container node\-pools create high\-mem\-pool \e
    \-\-machine\-type=custom\-2\-12288
.RE

.TP 2m
\fB\-\-max\-pods\-per\-node\fR=\fIMAX_PODS_PER_NODE\fR
The max number of pods per node for this node pool.

This flag sets the maximum number of pods that can be run at the same time on a
node. This will override the value given with \-\-default\-max\-pods\-per\-node
flag set at the cluster level.

Must be used in conjunction with '\-\-enable\-ip\-alias'.

.TP 2m
\fB\-\-max\-surge\-upgrade\fR=\fIMAX_SURGE_UPGRADE\fR; default=1
Number of extra (surge) nodes to be created on each upgrade of the node pool.

Specifies the number of extra (surge) nodes to be created during this node
pool's upgrades. For example, running the following command will result in
creating an extra node each time the node pool is upgraded:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \-\-max\-surge\-upgrade=1   \e
    \-\-max\-unavailable\-upgrade=0
.RE

Must be used in conjunction with '\-\-max\-unavailable\-upgrade'.

.TP 2m
\fB\-\-max\-unavailable\-upgrade\fR=\fIMAX_UNAVAILABLE_UPGRADE\fR
Number of nodes that can be unavailable at the same time on each upgrade of the
node pool.

Specifies the number of nodes that can be unavailable at the same time during
this node pool's upgrades. For example, running the following command will
result in having 3 nodes being upgraded in parallel (1 + 2), but keeping always
at least 3 (5 \- 2) available each time the node pool is upgraded:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \-\-num\-nodes=5   \e
    \-\-max\-surge\-upgrade=1 \-\-max\-unavailable\-upgrade=2
.RE

Must be used in conjunction with '\-\-max\-surge\-upgrade'.

.TP 2m
\fB\-\-metadata\fR=\fIKEY\fR=\fIVALUE\fR,[\fIKEY\fR=\fIVALUE\fR,...]
Compute Engine metadata to be made available to the guest operating system
running on nodes within the node pool.

Each metadata entry is a key/value pair separated by an equals sign. Metadata
keys must be unique and less than 128 bytes in length. Values must be less than
or equal to 32,768 bytes in length. The total size of all keys and values must
be less than 512 KB. Multiple arguments can be passed to this flag. For example:

\f5\fI\-\-metadata key\-1=value\-1,key\-2=value\-2,key\-3=value\-3\fR\fR

Additionally, the following keys are reserved for use by Kubernetes Engine:

.RS 2m
.IP "\(em" 2m
\f5\fIcluster\-location\fR\fR
.IP "\(em" 2m
\f5\fIcluster\-name\fR\fR
.IP "\(em" 2m
\f5\fIcluster\-uid\fR\fR
.IP "\(em" 2m
\f5\fIconfigure\-sh\fR\fR
.IP "\(em" 2m
\f5\fIenable\-os\-login\fR\fR
.IP "\(em" 2m
\f5\fIgci\-update\-strategy\fR\fR
.IP "\(em" 2m
\f5\fIgci\-ensure\-gke\-docker\fR\fR
.IP "\(em" 2m
\f5\fIinstance\-template\fR\fR
.IP "\(em" 2m
\f5\fIkube\-env\fR\fR
.IP "\(em" 2m
\f5\fIstartup\-script\fR\fR
.IP "\(em" 2m
\f5\fIuser\-data\fR\fR

.RE
.RE
.sp
See also Compute Engine's documentation
(https://cloud.google.com/compute/docs/storing\-retrieving\-metadata) on storing
and retrieving instance metadata.

.RS 2m
.TP 2m
\fB\-\-metadata\-from\-file\fR=\fIKEY\fR=\fILOCAL_FILE_PATH\fR,[...]
Same as \f5\fI\-\-metadata\fR\fR except that the value for the entry will be
read from a local file.

.TP 2m
\fB\-\-min\-cpu\-platform\fR=\fIPLATFORM\fR
When specified, the nodes for the new node pool will be scheduled on host with
specified CPU architecture or a newer one.

Examples:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \-\-min\-cpu\-platform=PLATFORM
.RE

To list available CPU platforms in given zone, run:

.RS 2m
$ gcloud beta compute zones describe ZONE \e
    \-\-format="value(availableCpuPlatforms)"
.RE

CPU platform selection is available only in selected zones.

.TP 2m
\fB\-\-node\-group\fR=\fINODE_GROUP\fR
Assign instances of this pool to run on the specified Google Compute Engine node
group. This is useful for running workloads on sole tenant nodes.

To see available sole tenant node\-groups, run:

.RS 2m
$ gcloud compute sole\-tenancy node\-groups list
.RE

To create a sole tenant node group, run:

.RS 2m
$ gcloud compute sole\-tenancy node\-groups create [GROUP_NAME]     \e
    \-\-zone [ZONE] \-\-node\-template [TEMPLATE_NAME] \e
    \-\-target\-size [TARGET_SIZE]
.RE

See https://cloud.google.com/compute/docs/nodes for more information on sole
tenancy and node groups.

.TP 2m
\fB\-\-node\-labels\fR=[\fINODE_LABEL\fR,...]
Applies the given kubernetes labels on all nodes in the new node pool. Example:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \e
    \-\-node\-labels=label1=value1,label2=value2
.RE

New nodes, including ones created by resize or recreate, will have these labels
on the kubernetes API node object and can be used in nodeSelectors. See
http://kubernetes.io/docs/user\-guide/node\-selection/ for examples.

Note that kubernetes labels, intended to associate cluster components and
resources with one another and manage resource lifecycles, are different from
Kubernetes Engine labels that are used for the purpose of tracking billing and
usage information.

.TP 2m
\fB\-\-node\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]
The set of zones in which the node pool's nodes should be located.

Multiple locations can be specified, separated by commas. For example:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-node\-locations=us\-central1\-a,us\-central1\-b
.RE

.TP 2m
\fB\-\-node\-taints\fR=[\fINODE_TAINT\fR,...]
Applies the given kubernetes taints on all nodes in the new node pool, which can
be used with tolerations for pod scheduling. Example:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \e
    \-\-node\-taints=key1=val1:NoSchedule,key2=val2:PreferNoSchedule
.RE

Note, this feature uses \f5gcloud beta\fR commands. To use gcloud beta commands,
you must configure \f5gcloud\fR to use the v1beta1 API as described here:
https://cloud.google.com/kubernetes\-engine/docs/reference/api\-organization#beta.
To read more about node\-taints, see
https://cloud.google.com/kubernetes\-engine/docs/node\-taints.

.TP 2m
\fB\-\-node\-version\fR=\fINODE_VERSION\fR
The Kubernetes version to use for nodes. Defaults to server\-specified.

The default Kubernetes version is available using the following command.

.RS 2m
$ gcloud container get\-server\-config
.RE

.TP 2m
\fB\-\-num\-nodes\fR=\fINUM_NODES\fR; default=3
The number of nodes in the node pool in each of the cluster's zones.

.TP 2m
\fB\-\-preemptible\fR
Create nodes using preemptible VM instances in the new node pool.

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \-\-preemptible
.RE

New nodes, including ones created by resize or recreate, will use preemptible VM
instances. See https://cloud.google.com/kubernetes\-engine/docs/preemptible\-vm
for more information on how to use Preemptible VMs with Kubernetes Engine.

.TP 2m
\fB\-\-sandbox\fR=[\fItype\fR=\fITYPE\fR]
Enables the requested sandbox on all nodes in the node pool. Example:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \-\-sandbox="type=gvisor"
.RE

The only supported type is 'gvisor'.

.TP 2m
\fB\-\-shielded\-integrity\-monitoring\fR
Enables monitoring and attestation of the boot integrity of the instance. The
attestation is performed against the integrity policy baseline. This baseline is
initially derived from the implicitly trusted boot image when the instance is
created.

.TP 2m
\fB\-\-shielded\-secure\-boot\fR
The instance will boot with secure boot enabled.

.TP 2m
\fB\-\-system\-config\-from\-file\fR=\fISYSTEM_CONFIG_FROM_FILE\fR
Path of the YAML/JSON file that contains the node configuration, including Linux
kernel parameters (sysctls) and kubelet configs.

Example:

.RS 2m
kubeletConfig:
  cpuManagerPolicy: static
linuxConfig:
  sysctl:
    net.core.somaxconn: '2048'
    net.ipv4.tcp_rmem: '4096 87380 6291456'
.RE

List of supported kubelet configs in 'kubeletConfig'.


.TS
tab(	);
l(17)B l(34)B
l(17) l(34).
KEY	VALUE
cpuManagerPolicy	either 'static' or 'default'
cpuCFSQuota	true or false (enabled by default)
cpuCFSQuotaPeriod	interval (e.g., '100ms')
.TE

List of supported sysctls in 'linuxConfig'.


.TS
tab(	);
l(42)B l(34)B
l(42) l(34).
KEY	VALUE
kernel.pid_max	Must be [32768, 4194304]
fs.inotify.max_queued_events	Must be [16384, 4194304]
fs.inotify.max_user_instances	Must be [128, 4194304]
fs.inotify.max_user_watches	Must be [12288, 4194304]
net.core.netdev_budget	Any positive integer
net.core.netdev_budget_usecs	Any positive integer
net.core.netdev_max_backlog	Any positive integer
net.core.rmem_default	Any positive integer
net.core.rmem_max	Any positive integer
net.core.wmem_default	Any positive integer
net.core.wmem_max	Any positive integer
net.core.optmem_max	Any positive integer
net.core.somaxconn	Must be [128, 4194304]
net.ipv4.tcp_rmem	Any positive integer
net.ipv4.tcp_wmem	Any positive integer tuple
net.ipv4.tcp_mem	Any positive integer
net.ipv4.tcp_fin_timeout	Any positive integer
net.ipv4.tcp_keepalive_intvl	Any positive integer
net.ipv4.tcp_keepalive_probes	Any positive integer
net.ipv4.tcp_keepalive_time	Any positive integer
net.ipv4.tcp_max_orphans	Any positive integer
net.ipv4.tcp_max_syn_backlog	Any positive integer
net.ipv4.tcp_max_tw_buckets	Any positive integer
net.ipv4.tcp_syn_retries	Any positive integer
net.ipv4.tcp_tw_reuse	Must be {0, 1}
net.ipv4.udp_mem	Any positive integer tuple
net.ipv4.udp_rmem_min	Any positive integer
net.ipv4.udp_wmem_min	Any positive integer
net.netfilter.nf_conntrack_generic_timeout	Any positive integer
.TE

.TP 2m
\fB\-\-tags\fR=\fITAG\fR,[\fITAG\fR,...]
Applies the given Compute Engine tags (comma separated) on all nodes in the new
node\-pool. Example:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \-\-tags=tag1,tag2
.RE

New nodes, including ones created by resize or recreate, will have these tags on
the Compute Engine API instance object and can be used in firewall rules. See
https://cloud.google.com/sdk/gcloud/reference/compute/firewall\-rules/create for
examples.

.TP 2m
\fB\-\-workload\-metadata\fR=\fIWORKLOAD_METADATA\fR
Type of metadata server available to pods running in the node pool.
\fIWORKLOAD_METADATA\fR must be one of:

.RS 2m
.TP 2m
\fBEXPOSED\fR
[DEPRECATED] Pods running in this node pool have access to the node's underlying
Compute Engine Metadata Server.
.TP 2m
\fBGCE_METADATA\fR
Pods running in this node pool have access to the node's underlying Compute
Engine Metadata Server.
.TP 2m
\fBGKE_METADATA\fR
Run the Kubernetes Engine Metadata Server on this node. The Kubernetes Engine
Metadata Server exposes a metadata API to workloads that is compatible with the
V1 Compute Metadata APIs exposed by the Compute Engine and App Engine Metadata
Servers. This feature can only be enabled if Workload Identity is enabled at the
cluster level.
.TP 2m
\fBGKE_METADATA_SERVER\fR
[DEPRECATED] Run the Kubernetes Engine Metadata Server on this node. The
Kubernetes Engine Metadata Server exposes a metadata API to workloads that is
compatible with the V1 Compute Metadata APIs exposed by the Compute Engine and
App Engine Metadata Servers. This feature can only be enabled if Workload
Identity is enabled at the cluster level.
.TP 2m
\fBSECURE\fR
[DPRECATED] Prevents pods not in hostNetwork from accessing certain VM metadata,
specifically kube\-env, which contains Kubelet credentials, and the instance
identity token. This is a temporary security solution available while the
bootstrapping process for cluster nodes is being redesigned with significant
security improvements. This feature is scheduled to be deprecated in the future
and later removed.
.RE
.sp


.TP 2m

Cluster autoscaling

.RS 2m
.TP 2m
\fB\-\-enable\-autoscaling\fR
Enables autoscaling for a node pool.

Enables autoscaling in the node pool specified by \-\-node\-pool or the default
node pool if \-\-node\-pool is not provided.

.TP 2m
\fB\-\-max\-nodes\fR=\fIMAX_NODES\fR
Maximum number of nodes in the node pool.

Maximum number of nodes to which the node pool specified by \-\-node\-pool (or
default node pool if unspecified) can scale. Ignored unless
\-\-enable\-autoscaling is also specified.

.TP 2m
\fB\-\-min\-nodes\fR=\fIMIN_NODES\fR
Minimum number of nodes in the node pool.

Minimum number of nodes to which the node pool specified by \-\-node\-pool (or
default node pool if unspecified) can scale. Ignored unless
\-\-enable\-autoscaling is also specified.

.RE
.sp
.TP 2m

At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-local\-ssd\-count\fR=\fILOCAL_SSD_COUNT\fR
\-\-local\-ssd\-volumes enables the ability to request local SSD with variable
count, interfaces, and format

\-\-local\-ssd\-count is the equivalent of using \-\-local\-ssd\-volumes with
type=scsi,format=fs

The number of local SSD disks to provision on each node.

Local SSDs have a fixed 375 GB capacity per device. The number of disks that can
be attached to an instance is limited by the maximum number of disks available
on a machine, which differs by compute zone. See
https://cloud.google.com/compute/docs/disks/local\-ssd for more information.

.TP 2m
\fB\-\-local\-ssd\-volumes\fR=[[\fIcount\fR=\fICOUNT\fR],[\fItype\fR=\fITYPE\fR],[\fIformat\fR=\fIFORMAT\fR],...]
\-\-local\-ssd\-volumes enables the ability to request local SSD with variable
count, interfaces, and format

\-\-local\-ssd\-count is the equivalent of using \-\-local\-ssd\-volumes with
type=scsi,format=fs

Adds the requested local SSDs on all nodes in default node pool(s) in new
cluster. Example:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \e
    \-\-local\-ssd\-volumes count=2,type=nvme,format=fs
.RE

\'count' must be between 1\-8

\'type' must be either scsi or nvme

\'format' must be either fs or block

New nodes, including ones created by resize or recreate, will have these local
SSDs.

Local SSDs have a fixed 375 GB capacity per device. The number of disks that can
be attached to an instance is limited by the maximum number of disks available
on a machine, which differs by compute zone. See
https://cloud.google.com/compute/docs/disks/local\-ssd for more information.

.RE
.sp
.TP 2m

At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-region\fR=\fIREGION\fR
Compute region (e.g. us\-central1) for the cluster.

.TP 2m
\fB\-\-zone\fR=\fIZONE\fR, \fB\-z\fR \fIZONE\fR
Compute zone (e.g. us\-central1\-a) for the cluster. Overrides the default
\fBcompute/zone\fR property value for this command invocation.

.RE
.sp
.TP 2m

Specifies the reservation for the node pool.

.RS 2m
.TP 2m
\fB\-\-reservation\fR=\fIRESERVATION\fR
The name of the reservation, required when
\f5\-\-reservation\-affinity=specific\fR.

.TP 2m
\fB\-\-reservation\-affinity\fR=\fIRESERVATION_AFFINITY\fR
The type of the reservation for the node pool. \fIRESERVATION_AFFINITY\fR must
be one of: \fBany\fR, \fBnone\fR, \fBspecific\fR.

.RE
.sp
.TP 2m

Options to specify the node identity.

.RS 2m
.TP 2m

Scopes options.

.RS 2m
.TP 2m
\fB\-\-scopes\fR=[\fISCOPE\fR,...]; default="gke\-default"
Specifies scopes for the node instances. Examples:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \e
    \-\-scopes=https://www.googleapis.com/auth/devstorage.read_only
.RE

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=example\-cluster \e
    \-\-scopes=bigquery,storage\-rw,compute\-ro
.RE

Multiple SCOPEs can be specified, separated by commas. \f5logging\-write\fR
and/or \f5monitoring\fR are added unless Cloud Logging and/or Cloud Monitoring
are disabled (see \f5\-\-enable\-cloud\-logging\fR and
\f5\-\-enable\-cloud\-monitoring\fR for more information). SCOPE can be either
the full URI of the scope or an alias. \fBdefault\fR scopes are assigned to all
instances. Available aliases are:


.TS
tab(	);
lB lB
l l.
Alias	URI
bigquery	https://www.googleapis.com/auth/bigquery
cloud-platform	https://www.googleapis.com/auth/cloud-platform
cloud-source-repos	https://www.googleapis.com/auth/source.full_control
cloud-source-repos-ro	https://www.googleapis.com/auth/source.read_only
compute-ro	https://www.googleapis.com/auth/compute.readonly
compute-rw	https://www.googleapis.com/auth/compute
datastore	https://www.googleapis.com/auth/datastore
default	https://www.googleapis.com/auth/devstorage.read_only
	https://www.googleapis.com/auth/logging.write
	https://www.googleapis.com/auth/monitoring.write
	https://www.googleapis.com/auth/pubsub
	https://www.googleapis.com/auth/service.management.readonly
	https://www.googleapis.com/auth/servicecontrol
	https://www.googleapis.com/auth/trace.append
gke-default	https://www.googleapis.com/auth/devstorage.read_only
	https://www.googleapis.com/auth/logging.write
	https://www.googleapis.com/auth/monitoring
	https://www.googleapis.com/auth/service.management.readonly
	https://www.googleapis.com/auth/servicecontrol
	https://www.googleapis.com/auth/trace.append
logging-write	https://www.googleapis.com/auth/logging.write
monitoring	https://www.googleapis.com/auth/monitoring
monitoring-write	https://www.googleapis.com/auth/monitoring.write
pubsub	https://www.googleapis.com/auth/pubsub
service-control	https://www.googleapis.com/auth/servicecontrol
service-management	https://www.googleapis.com/auth/service.management.readonly
sql (deprecated)	https://www.googleapis.com/auth/sqlservice
sql-admin	https://www.googleapis.com/auth/sqlservice.admin
storage-full	https://www.googleapis.com/auth/devstorage.full_control
storage-ro	https://www.googleapis.com/auth/devstorage.read_only
storage-rw	https://www.googleapis.com/auth/devstorage.read_write
taskqueue	https://www.googleapis.com/auth/taskqueue
trace	https://www.googleapis.com/auth/trace.append
userinfo-email	https://www.googleapis.com/auth/userinfo.email
.TE

DEPRECATION WARNING: https://www.googleapis.com/auth/sqlservice account scope
and \f5sql\fR alias do not provide SQL instance management capabilities and have
been deprecated. Please, use https://www.googleapis.com/auth/sqlservice.admin or
\f5sql\-admin\fR to manage your Google SQL Service instances.


.RE
.sp
.TP 2m
\fB\-\-service\-account\fR=\fISERVICE_ACCOUNT\fR
The Google Cloud Platform Service Account to be used by the node VMs. If a
service account is specified, the cloud\-platform and userinfo.email scopes are
used. If no Service Account is specified, the project default service account is
used.


.RE
.RE
.sp

.SH "GCLOUD WIDE FLAGS"

These flags are available to all commands: \-\-account, \-\-billing\-project,
\-\-configuration, \-\-flags\-file, \-\-flatten, \-\-format, \-\-help,
\-\-impersonate\-service\-account, \-\-log\-http, \-\-project, \-\-quiet,
\-\-trace\-token, \-\-user\-output\-enabled, \-\-verbosity.

Run \fB$ gcloud help\fR for details.



.SH "EXAMPLES"

To create a new node pool "node\-pool\-1" with the default options in the
cluster "sample\-cluster", run:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=sample\-cluster
.RE

The new node pool will show up in the cluster after all the nodes have been
provisioned.

To create a node pool with 5 nodes, run:

.RS 2m
$ gcloud alpha container node\-pools create node\-pool\-1 \e
    \-\-cluster=sample\-cluster \-\-num\-nodes=5
.RE



.SH "NOTES"

This command is currently in ALPHA and may change without notice. If this
command fails with API permission errors despite specifying the right project,
you may be trying to access an API with an invitation\-only early access
whitelist. These variants are also available:

.RS 2m
$ gcloud container node\-pools create
$ gcloud beta container node\-pools create
.RE

