
.TH "GCLOUD_CONTAINER_CLUSTERS_CREATE" 1



.SH "NAME"
.HP
gcloud container clusters create \- create a cluster for running containers



.SH "SYNOPSIS"
.HP
\f5gcloud container clusters create\fR \fINAME\fR [\fB\-\-accelerator\fR=[\fItype\fR=\fITYPE\fR,[\fIcount\fR=\fICOUNT\fR],...]] [\fB\-\-additional\-zones\fR=\fIZONE\fR,[\fIZONE\fR,...]] [\fB\-\-addons\fR=[\fIADDON\fR,...]] [\fB\-\-async\fR] [\fB\-\-boot\-disk\-kms\-key\fR=\fIBOOT_DISK_KMS_KEY\fR] [\fB\-\-cloud\-run\-config\fR=[\fIload\-balancer\-type\fR=\fIEXTERNAL\fR,...]] [\fB\-\-cluster\-ipv4\-cidr\fR=\fICLUSTER_IPV4_CIDR\fR] [\fB\-\-cluster\-secondary\-range\-name\fR=\fINAME\fR] [\fB\-\-cluster\-version\fR=\fICLUSTER_VERSION\fR] [\fB\-\-create\-subnetwork\fR=[\fIKEY\fR=\fIVALUE\fR,...]] [\fB\-\-database\-encryption\-key\fR=\fIDATABASE_ENCRYPTION_KEY\fR] [\fB\-\-default\-max\-pods\-per\-node\fR=\fIDEFAULT_MAX_PODS_PER_NODE\fR] [\fB\-\-disable\-default\-snat\fR] [\fB\-\-disk\-size\fR=\fIDISK_SIZE\fR] [\fB\-\-disk\-type\fR=\fIDISK_TYPE\fR] [\fB\-\-enable\-autorepair\fR] [\fB\-\-no\-enable\-autoupgrade\fR] [\fB\-\-enable\-binauthz\fR] [\fB\-\-enable\-cloud\-logging\fR] [\fB\-\-enable\-cloud\-monitoring\fR] [\fB\-\-enable\-cloud\-run\-alpha\fR] [\fB\-\-enable\-intra\-node\-visibility\fR] [\fB\-\-enable\-ip\-alias\fR] [\fB\-\-enable\-kubernetes\-alpha\fR] [\fB\-\-enable\-legacy\-authorization\fR] [\fB\-\-enable\-master\-global\-access\fR] [\fB\-\-enable\-network\-policy\fR] [\fB\-\-enable\-shielded\-nodes\fR] [\fB\-\-enable\-stackdriver\-kubernetes\fR] [\fB\-\-enable\-vertical\-pod\-autoscaling\fR] [\fB\-\-image\-type\fR=\fIIMAGE_TYPE\fR] [\fB\-\-issue\-client\-certificate\fR] [\fB\-\-labels\fR=[\fIKEY\fR=\fIVALUE\fR,...]] [\fB\-\-local\-ssd\-count\fR=\fILOCAL_SSD_COUNT\fR] [\fB\-\-machine\-type\fR=\fIMACHINE_TYPE\fR,\ \fB\-m\fR\ \fIMACHINE_TYPE\fR] [\fB\-\-max\-nodes\-per\-pool\fR=\fIMAX_NODES_PER_POOL\fR] [\fB\-\-max\-pods\-per\-node\fR=\fIMAX_PODS_PER_NODE\fR] [\fB\-\-max\-surge\-upgrade\fR=\fIMAX_SURGE_UPGRADE\fR] [\fB\-\-max\-unavailable\-upgrade\fR=\fIMAX_UNAVAILABLE_UPGRADE\fR] [\fB\-\-metadata\fR=\fIKEY\fR=\fIVALUE\fR,[\fIKEY\fR=\fIVALUE\fR,...]] [\fB\-\-metadata\-from\-file\fR=\fIKEY\fR=\fILOCAL_FILE_PATH\fR,[...]] [\fB\-\-min\-cpu\-platform\fR=\fIPLATFORM\fR] [\fB\-\-network\fR=\fINETWORK\fR] [\fB\-\-node\-labels\fR=[\fINODE_LABEL\fR,...]] [\fB\-\-node\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]] [\fB\-\-node\-taints\fR=[\fINODE_TAINT\fR,...]] [\fB\-\-node\-version\fR=\fINODE_VERSION\fR] [\fB\-\-num\-nodes\fR=\fINUM_NODES\fR;\ default=3] [\fB\-\-preemptible\fR] [\fB\-\-release\-channel\fR=\fICHANNEL\fR] [\fB\-\-services\-ipv4\-cidr\fR=\fICIDR\fR] [\fB\-\-services\-secondary\-range\-name\fR=\fINAME\fR] [\fB\-\-shielded\-integrity\-monitoring\fR] [\fB\-\-shielded\-secure\-boot\fR] [\fB\-\-subnetwork\fR=\fISUBNETWORK\fR] [\fB\-\-tags\fR=\fITAG\fR,[\fITAG\fR,...]] [\fB\-\-workload\-metadata\fR=\fIWORKLOAD_METADATA\fR] [\fB\-\-workload\-pool\fR=\fIWORKLOAD_POOL\fR] [[\fB\-\-enable\-autoprovisioning\fR\ :\ \fB\-\-autoprovisioning\-config\-file\fR=\fIAUTOPROVISIONING_CONFIG_FILE\fR\ |\ [\fB\-\-max\-cpu\fR=\fIMAX_CPU\fR\ \fB\-\-max\-memory\fR=\fIMAX_MEMORY\fR\ :\ \fB\-\-autoprovisioning\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]\ \fB\-\-autoprovisioning\-min\-cpu\-platform\fR=\fIPLATFORM\fR\ \fB\-\-min\-cpu\fR=\fIMIN_CPU\fR\ \fB\-\-min\-memory\fR=\fIMIN_MEMORY\fR\ \fB\-\-autoprovisioning\-max\-surge\-upgrade\fR=\fIAUTOPROVISIONING_MAX_SURGE_UPGRADE\fR\ \fB\-\-autoprovisioning\-max\-unavailable\-upgrade\fR=\fIAUTOPROVISIONING_MAX_UNAVAILABLE_UPGRADE\fR\ \fB\-\-autoprovisioning\-scopes\fR=[\fISCOPE\fR,...]\ \fB\-\-autoprovisioning\-service\-account\fR=\fIAUTOPROVISIONING_SERVICE_ACCOUNT\fR\ \fB\-\-enable\-autoprovisioning\-autorepair\fR\ \fB\-\-enable\-autoprovisioning\-autoupgrade\fR\ [\fB\-\-max\-accelerator\fR=[\fItype\fR=\fITYPE\fR,\fIcount\fR=\fICOUNT\fR,...]\ :\ \fB\-\-min\-accelerator\fR=[\fItype\fR=\fITYPE\fR,\fIcount\fR=\fICOUNT\fR,...]]]]] [\fB\-\-enable\-autoscaling\fR\ \fB\-\-max\-nodes\fR=\fIMAX_NODES\fR\ \fB\-\-min\-nodes\fR=\fIMIN_NODES\fR] [\fB\-\-enable\-master\-authorized\-networks\fR\ \fB\-\-master\-authorized\-networks\fR=\fINETWORK\fR,[\fINETWORK\fR,...]] [\fB\-\-enable\-network\-egress\-metering\fR\ \fB\-\-enable\-resource\-consumption\-metering\fR\ \fB\-\-resource\-usage\-bigquery\-dataset\fR=\fIRESOURCE_USAGE_BIGQUERY_DATASET\fR] [\fB\-\-enable\-private\-endpoint\fR\ \fB\-\-enable\-private\-nodes\fR\ \fB\-\-master\-ipv4\-cidr\fR=\fIMASTER_IPV4_CIDR\fR] [\fB\-\-enable\-tpu\fR\ \fB\-\-tpu\-ipv4\-cidr\fR=\fICIDR\fR] [\fB\-\-maintenance\-window\fR=\fISTART_TIME\fR\ |\ \fB\-\-maintenance\-window\-end\fR=\fITIME_STAMP\fR\ \fB\-\-maintenance\-window\-recurrence\fR=\fIRRULE\fR\ \fB\-\-maintenance\-window\-start\fR=\fITIME_STAMP\fR] [\fB\-\-password\fR=\fIPASSWORD\fR\ \fB\-\-enable\-basic\-auth\fR\ |\ \fB\-\-username\fR=\fIUSERNAME\fR,\ \fB\-u\fR\ \fIUSERNAME\fR] [\fB\-\-region\fR=\fIREGION\fR\ |\ \fB\-\-zone\fR=\fIZONE\fR,\ \fB\-z\fR\ \fIZONE\fR] [\fB\-\-reservation\fR=\fIRESERVATION\fR\ \fB\-\-reservation\-affinity\fR=\fIRESERVATION_AFFINITY\fR] [\fB\-\-scopes\fR=[\fISCOPE\fR,...];\ default="gke\-default"\ \fB\-\-service\-account\fR=\fISERVICE_ACCOUNT\fR] [\fIGCLOUD_WIDE_FLAG\ ...\fR]



.SH "DESCRIPTION"

Create a cluster for running containers.



.SH "POSITIONAL ARGUMENTS"

.RS 2m
.TP 2m
\fINAME\fR
The name of the cluster to create.

The name may contain only lowercase alphanumerics and '\-', must start with a
letter and end with an alphanumeric, and must be no longer than 40 characters.


.RE
.sp

.SH "FLAGS"

.RS 2m
.TP 2m
\fB\-\-accelerator\fR=[\fItype\fR=\fITYPE\fR,[\fIcount\fR=\fICOUNT\fR],...]
Attaches accelerators (e.g. GPUs) to all nodes.

.RS 2m
.TP 2m
\fBtype\fR
(Required) The specific type (e.g. nvidia\-tesla\-k80 for nVidia Tesla K80) of
accelerator to attach to the instances. Use \f5gcloud compute accelerator\-types
list\fR to learn about all available accelerator types.

.TP 2m
\fBcount\fR
(Optional) The number of accelerators to attach to the instances. The default
value is 1.

.RE
.sp
.TP 2m
\fB\-\-additional\-zones\fR=\fIZONE\fR,[\fIZONE\fR,...]
(DEPRECATED) The set of additional zones in which the specified node footprint
should be replicated. All zones must be in the same region as the cluster's
primary zone. If additional\-zones is not specified, all nodes will be in the
cluster's primary zone.

Note that \f5NUM_NODES\fR nodes will be created in each zone, such that if you
specify \f5\-\-num\-nodes=4\fR and choose one additional zone, 8 nodes will be
created.

Multiple locations can be specified, separated by commas. For example:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-zone us\-central1\-a \e
    \-\-additional\-zones us\-central1\-b,us\-central1\-c
.RE

This flag is deprecated. Use \-\-node\-locations=PRIMARY_ZONE,[ZONE,...]
instead.

.TP 2m
\fB\-\-addons\fR=[\fIADDON\fR,...]
Addons
(https://cloud.google.com/kubernetes\-engine/docs/reference/rest/v1/projects.locations.clusters#Cluster.AddonsConfig)
are additional Kubernetes cluster components. Addons specified by this flag will
be enabled. The others will be disabled. Default addons: HttpLoadBalancing,
HorizontalPodAutoscaling. \fIADDON\fR must be one of: \fBHttpLoadBalancing\fR,
\fBHorizontalPodAutoscaling\fR, \fBKubernetesDashboard\fR, \fBNetworkPolicy\fR,
\fBCloudRun\fR, \fBNodeLocalDNS\fR, \fBConfigConnector\fR.

.TP 2m
\fB\-\-async\fR
Return immediately, without waiting for the operation in progress to complete.

.TP 2m
\fB\-\-boot\-disk\-kms\-key\fR=\fIBOOT_DISK_KMS_KEY\fR
The Customer Managed Encryption Key used to encrypt the boot disk attached to
each node in the node pool. This should be of the form
projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME].
For more information about protecting resources with Cloud KMS Keys please see:
https://cloud.google.com/compute/docs/disks/customer\-managed\-encryption

.TP 2m
\fB\-\-cloud\-run\-config\fR=[\fIload\-balancer\-type\fR=\fIEXTERNAL\fR,...]
Configurations for Cloud Run addon, requires \f5\-\-addons=CloudRun\fR for
create and \f5\-\-update\-addons=CloudRun=ENABLED\fR for update.

.RS 2m
.TP 2m
\fBload\-balancer\-type\fR
Optional Type of load\-balancer\-type EXTERNAL or INTERNAL Example:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-cloud\-run\-config=load\-balancer\-type=INTERNAL
.RE

.RE
.sp
.TP 2m
\fB\-\-cluster\-ipv4\-cidr\fR=\fICLUSTER_IPV4_CIDR\fR
The IP address range for the pods in this cluster in CIDR notation (e.g.
10.0.0.0/14). Prior to Kubernetes version 1.7.0 this must be a subset of
10.0.0.0/8; however, starting with version 1.7.0 can be any RFC 1918 IP range.

.TP 2m
\fB\-\-cluster\-secondary\-range\-name\fR=\fINAME\fR
Set the secondary range to be used as the source for pod IPs. Alias ranges will
be allocated from this secondary range. NAME must be the name of an existing
secondary range in the cluster subnetwork.

Must be used in conjunction with '\-\-enable\-ip\-alias'. Cannot be used with
\-\-create\-subnetwork.

.TP 2m
\fB\-\-cluster\-version\fR=\fICLUSTER_VERSION\fR
The Kubernetes version to use for the master and nodes. Defaults to
server\-specified.

The default Kubernetes version is available using the following command.

.RS 2m
$ gcloud container get\-server\-config
.RE

.TP 2m
\fB\-\-create\-subnetwork\fR=[\fIKEY\fR=\fIVALUE\fR,...]
Create a new subnetwork for the cluster. The name and range of the subnetwork
can be customized via optional 'name' and 'range' key\-value pairs.

\'name' specifies the name of the subnetwork to be created.

\'range' specifies the IP range for the new subnetwork. This can either be a
netmask size (e.g. '/20') or a CIDR range (e.g. '10.0.0.0/20'). If a netmask
size is specified, the IP is automatically taken from the free space in the
cluster's network.

Examples:

Create a new subnetwork with a default name and size.

.RS 2m
$ gcloud container clusters create \-\-create\-subnetwork ""
.RE

Create a new subnetwork named "my\-subnet" with netmask of size 21.

.RS 2m
$ gcloud container clusters create \e
\-\-create\-subnetwork name=my\-subnet,range=/21
.RE

Create a new subnetwork with a default name with the primary range of
10.100.0.0/16.

.RS 2m
$ gcloud container clusters create \e
\-\-create\-subnetwork range=10.100.0.0/16
.RE

Create a new subnetwork with the name "my\-subnet" with a default range.

.RS 2m
$ gcloud container clusters create \-\-create\-subnetwork name=my\-subnet
.RE

Can not be specified unless '\-\-enable\-ip\-alias' is also specified. Can not
be used in conjunction with the '\-\-subnetwork' option.

.TP 2m
\fB\-\-database\-encryption\-key\fR=\fIDATABASE_ENCRYPTION_KEY\fR
Enable Database Encryption.

Enable database encryption that will be used to encrypt Kubernetes Secrets at
the application layer. The key provided should be the resource ID in the format
of
\f5projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]\fR.
For more information, see
https://cloud.google.com/kubernetes\-engine/docs/how\-to/encrypting\-secrets.

.TP 2m
\fB\-\-default\-max\-pods\-per\-node\fR=\fIDEFAULT_MAX_PODS_PER_NODE\fR
The default max number of pods per node for node pools in the cluster.

This flag sets the default max\-pods\-per\-node for node pools in the cluster.
If \-\-max\-pods\-per\-node is not specified explicitly for a node pool, this
flag value will be used.

Must be used in conjunction with '\-\-enable\-ip\-alias'.

.TP 2m
\fB\-\-disable\-default\-snat\fR
Disable default source NAT rules applied in cluster nodes.

By default, network traffic sending from Pods to outside of VPC will get
masqueraded by the node external IP. When this flag is set, no default sNAT will
be enforced on the cluster. The flag must be set if the cluster uses privately
used public IPs.

Must be used in conjunction with \f5\-\-enable\-ip\-alias\fR and
\f5\-\-enable\-private\-nodes\fR.

.TP 2m
\fB\-\-disk\-size\fR=\fIDISK_SIZE\fR
Size for node VM boot disks. Defaults to 100GB.

.TP 2m
\fB\-\-disk\-type\fR=\fIDISK_TYPE\fR
Type of the node VM boot disk. Defaults to pd\-standard. \fIDISK_TYPE\fR must be
one of: \fBpd\-standard\fR, \fBpd\-ssd\fR.

.TP 2m
\fB\-\-enable\-autorepair\fR
Enable node autorepair feature for a cluster's default node pool(s).

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-enable\-autorepair
.RE

Node autorepair is enabled by default for clusters using COS, COS_CONTAINERD,
UBUNTU or UBUNTU_CONTAINERD as a base image, use \-\-no\-enable\-autorepair to
disable.

See https://cloud.google.com/kubernetes\-engine/docs/how\-to/node\-auto\-repair
for more info.

.TP 2m
\fB\-\-enable\-autoupgrade\fR
Sets autoupgrade feature for a cluster's default node pool(s).

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-enable\-autoupgrade
.RE

See https://cloud.google.com/kubernetes\-engine/docs/node\-auto\-upgrades for
more info.

Enabled by default, use \fB\-\-no\-enable\-autoupgrade\fR to disable.

.TP 2m
\fB\-\-enable\-binauthz\fR
Enable Binary Authorization for this cluster.

.TP 2m
\fB\-\-enable\-cloud\-logging\fR
(DEPRECATED) Automatically send logs from the cluster to the Google Cloud
Logging API. This flag is deprecated, use
\f5\-\-enable\-stackdriver\-kubernetes\fR instead.

From 1.14, legacy Stackdriver GKE logging is deprecated. Thus, flag
\f5\-\-enable\-cloud\-logging\fR is also deprecated. Please use
\f5\-\-enable\-stackdriver\-kubernetes\fR instead, to migrate to new Stackdriver
Kubernetes Engine monitoring and logging. For more details, please read:
https://cloud.google.com/monitoring/kubernetes\-engine/migration.

.TP 2m
\fB\-\-enable\-cloud\-monitoring\fR
(DEPRECATED) Automatically send metrics from pods in the cluster to the Google
Cloud Monitoring API. VM metrics will be collected by Google Compute Engine
regardless of this setting. This flag is deprecated, use
\f5\-\-enable\-stackdriver\-kubernetes\fR instead.

From 1.14, legacy Stackdriver GKE monitoring is deprecated. Thus, flag
\f5\-\-enable\-cloud\-monitoring\fR is also deprecated. Please use
\f5\-\-enable\-stackdriver\-kubernetes\fR instead, to migrate to new Stackdriver
Kubernetes Engine monitoring and logging. For more details, please read:
https://cloud.google.com/monitoring/kubernetes\-engine/migration.

.TP 2m
\fB\-\-enable\-cloud\-run\-alpha\fR
Enable Cloud Run alpha features on this cluster. Selecting this option will
result in the cluster having all Cloud Run alpha API groups and features turned
on.

Cloud Run alpha clusters are not covered by the Cloud Run SLA and should not be
used for production workloads.

.TP 2m
\fB\-\-enable\-intra\-node\-visibility\fR
Enable Intra\-node visibility for this cluster.

Enabling intra\-node visibility makes your intra\-node pod\-to\-pod traffic
visible to the networking fabric. With this feature, you can use VPC flow
logging or other VPC features for intra\-node traffic.

Enabling it on an existing cluster causes the cluster master and the cluster
nodes to restart, which might cause a disruption.

.TP 2m
\fB\-\-enable\-ip\-alias\fR
Enable use of alias IPs (https://cloud.google.com/compute/docs/alias\-ip/) for
Pod IPs. This will require at least two secondary ranges in the subnetwork, one
for the pod IPs and another to reserve space for the services range.

.TP 2m
\fB\-\-enable\-kubernetes\-alpha\fR
Enable Kubernetes alpha features on this cluster. Selecting this option will
result in the cluster having all Kubernetes alpha API groups and features turned
on. Cluster upgrades (both manual and automatic) will be disabled and the
cluster will be automatically deleted after 30 days.

Alpha clusters are not covered by the Kubernetes Engine SLA and should not be
used for production workloads.

.TP 2m
\fB\-\-enable\-legacy\-authorization\fR
Enables the legacy ABAC authentication for the cluster. User rights are granted
through the use of policies which combine attributes together. For a detailed
look at these properties and related formats, see
https://kubernetes.io/docs/admin/authorization/abac/. To use RBAC permissions
instead, create or update your cluster with the option
\f5\-\-no\-enable\-legacy\-authorization\fR.

.TP 2m
\fB\-\-enable\-master\-global\-access\fR
Use with private clusters to allow access to the master's private endpoint from
any Google Cloud region or on\-premises environment regardless of the private
cluster's region.

Must be used in conjunction with '\-\-enable\-ip\-alias' and
\'\-\-enable\-private\-nodes'.

.TP 2m
\fB\-\-enable\-network\-policy\fR
Enable network policy enforcement for this cluster. If you are enabling network
policy on an existing cluster the network policy addon must first be enabled on
the master by using \-\-update\-addons=NetworkPolicy=ENABLED flag.

.TP 2m
\fB\-\-enable\-shielded\-nodes\fR
Enable Shielded Nodes for this cluster. Enabling Shielded Nodes will enable a
more secure Node credential bootstrapping implementation. Starting with version
1.18, clusters will have shielded GKE nodes by default.

.TP 2m
\fB\-\-enable\-stackdriver\-kubernetes\fR
Enable Stackdriver Kubernetes monitoring and logging.

.TP 2m
\fB\-\-enable\-vertical\-pod\-autoscaling\fR
Enable vertical pod autoscaling for a cluster.

.TP 2m
\fB\-\-image\-type\fR=\fIIMAGE_TYPE\fR
The image type to use for the cluster. Defaults to server\-specified.

Image Type specifies the base OS that the nodes in the cluster will run on. If
an image type is specified, that will be assigned to the cluster and all future
upgrades will use the specified image type. If it is not specified the server
will pick the default image type.

The default image type and the list of valid image types are available using the
following command.

.RS 2m
$ gcloud container get\-server\-config
.RE

.TP 2m
\fB\-\-issue\-client\-certificate\fR
Issue a TLS client certificate with admin permissions.

When enabled, the certificate and private key pair will be present in MasterAuth
field of the Cluster object. For cluster versions before 1.12, a client
certificate will be issued by default. As of 1.12, client certificates are
disabled by default.

.TP 2m
\fB\-\-labels\fR=[\fIKEY\fR=\fIVALUE\fR,...]
Labels to apply to the Google Cloud resources in use by the Kubernetes Engine
cluster. These are unrelated to Kubernetes labels. Example:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-labels=label_a=value1,label_b=,label_c=value3
.RE

.TP 2m
\fB\-\-local\-ssd\-count\fR=\fILOCAL_SSD_COUNT\fR
The number of local SSD disks to provision on each node.

Local SSDs have a fixed 375 GB capacity per device. The number of disks that can
be attached to an instance is limited by the maximum number of disks available
on a machine, which differs by compute zone. See
https://cloud.google.com/compute/docs/disks/local\-ssd for more information.

.TP 2m
\fB\-\-machine\-type\fR=\fIMACHINE_TYPE\fR, \fB\-m\fR \fIMACHINE_TYPE\fR
The type of machine to use for nodes. Defaults to e2\-medium. The list of
predefined machine types is available using the following command:

.RS 2m
$ gcloud compute machine\-types list
.RE

You can also specify custom machine types with the string "custom\-CPUS\-RAM"
where \f5CPUS\fR is the number of virtual CPUs and \f5RAM\fR is the amount of
RAM in MiB.

For example, to create a node pool using custom machines with 2 vCPUs and 12 GB
of RAM:

.RS 2m
$ gcloud container clusters create high\-mem\-pool \e
    \-\-machine\-type=custom\-2\-12288
.RE

.TP 2m
\fB\-\-max\-nodes\-per\-pool\fR=\fIMAX_NODES_PER_POOL\fR
The maximum number of nodes to allocate per default initial node pool.
Kubernetes Engine will automatically create enough nodes pools such that each
node pool contains less than \f5\-\-max\-nodes\-per\-pool\fR nodes. Defaults to
1000 nodes, but can be set as low as 100 nodes per pool on initial create.

.TP 2m
\fB\-\-max\-pods\-per\-node\fR=\fIMAX_PODS_PER_NODE\fR
The max number of pods per node for this node pool.

This flag sets the maximum number of pods that can be run at the same time on a
node. This will override the value given with \-\-default\-max\-pods\-per\-node
flag set at the cluster level.

Must be used in conjunction with '\-\-enable\-ip\-alias'.

.TP 2m
\fB\-\-max\-surge\-upgrade\fR=\fIMAX_SURGE_UPGRADE\fR
Number of extra (surge) nodes to be created on each upgrade of a node pool.

Specifies the number of extra (surge) nodes to be created during this node
pool's upgrades. For example, running the following command will result in
creating an extra node each time the node pool is upgraded:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-max\-surge\-upgrade=1 \-\-max\-unavailable\-upgrade=0
.RE

Must be used in conjunction with '\-\-max\-unavailable\-upgrade'.

.TP 2m
\fB\-\-max\-unavailable\-upgrade\fR=\fIMAX_UNAVAILABLE_UPGRADE\fR
Number of nodes that can be unavailable at the same time on each upgrade of a
node pool.

Specifies the number of nodes that can be unavailable at the same time while
this node pool is being upgraded. For example, running the following command
will result in having 3 nodes being upgraded in parallel (1 + 2), but keeping
always at least 3 (5 \- 2) available each time the node pool is upgraded:

.RS 2m
$ gcloud container clusters create example\-cluster \-\-num\-nodes=5 \e
   \-\-max\-surge\-upgrade=1      \-\-max\-unavailable\-upgrade=2
.RE

Must be used in conjunction with '\-\-max\-surge\-upgrade'.

.TP 2m
\fB\-\-metadata\fR=\fIKEY\fR=\fIVALUE\fR,[\fIKEY\fR=\fIVALUE\fR,...]
Compute Engine metadata to be made available to the guest operating system
running on nodes within the node pool.

Each metadata entry is a key/value pair separated by an equals sign. Metadata
keys must be unique and less than 128 bytes in length. Values must be less than
or equal to 32,768 bytes in length. The total size of all keys and values must
be less than 512 KB. Multiple arguments can be passed to this flag. For example:

\f5\fI\-\-metadata key\-1=value\-1,key\-2=value\-2,key\-3=value\-3\fR\fR

Additionally, the following keys are reserved for use by Kubernetes Engine:

.RS 2m
.IP "\(em" 2m
\f5\fIcluster\-location\fR\fR
.IP "\(em" 2m
\f5\fIcluster\-name\fR\fR
.IP "\(em" 2m
\f5\fIcluster\-uid\fR\fR
.IP "\(em" 2m
\f5\fIconfigure\-sh\fR\fR
.IP "\(em" 2m
\f5\fIenable\-os\-login\fR\fR
.IP "\(em" 2m
\f5\fIgci\-update\-strategy\fR\fR
.IP "\(em" 2m
\f5\fIgci\-ensure\-gke\-docker\fR\fR
.IP "\(em" 2m
\f5\fIinstance\-template\fR\fR
.IP "\(em" 2m
\f5\fIkube\-env\fR\fR
.IP "\(em" 2m
\f5\fIstartup\-script\fR\fR
.IP "\(em" 2m
\f5\fIuser\-data\fR\fR

.RE
.RE
.sp
Google Kubernetes Engine sets the following keys by default:

.RS 2m
.IP "\(bu" 2m
\f5\fIserial\-port\-logging\-enable\fR\fR

.RE
.sp
See also Compute Engine's documentation
(https://cloud.google.com/compute/docs/storing\-retrieving\-metadata) on storing
and retrieving instance metadata.

.RS 2m
.TP 2m
\fB\-\-metadata\-from\-file\fR=\fIKEY\fR=\fILOCAL_FILE_PATH\fR,[...]
Same as \f5\fI\-\-metadata\fR\fR except that the value for the entry will be
read from a local file.

.TP 2m
\fB\-\-min\-cpu\-platform\fR=\fIPLATFORM\fR
When specified, the nodes for the new cluster's default node pool will be
scheduled on host with specified CPU architecture or a newer one.

Examples:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-min\-cpu\-platform=PLATFORM
.RE

To list available CPU platforms in given zone, run:

.RS 2m
$ gcloud beta compute zones describe ZONE \e
    \-\-format="value(availableCpuPlatforms)"
.RE

CPU platform selection is available only in selected zones.

.TP 2m
\fB\-\-network\fR=\fINETWORK\fR
The Compute Engine Network that the cluster will connect to. Google Kubernetes
Engine will use this network when creating routes and firewalls for the
clusters. Defaults to the 'default' network.

.TP 2m
\fB\-\-node\-labels\fR=[\fINODE_LABEL\fR,...]
Applies the given kubernetes labels on all nodes in the new node pool. Example:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-node\-labels=label\-a=value1,label\-2=value2
.RE

New nodes, including ones created by resize or recreate, will have these labels
on the kubernetes API node object and can be used in nodeSelectors. See
http://kubernetes.io/docs/user\-guide/node\-selection/ for examples.

Note that kubernetes labels, intended to associate cluster components and
resources with one another and manage resource lifecycles, are different from
Kubernetes Engine labels that are used for the purpose of tracking billing and
usage information.

.TP 2m
\fB\-\-node\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]
The set of zones in which the specified node footprint should be replicated. All
zones must be in the same region as the cluster's master(s), specified by the
\f5\-\-zone\fR or \f5\-\-region\fR flag. Additionally, for zonal clusters,
\f5\-\-node\-locations\fR must contain the cluster's primary zone. If not
specified, all nodes will be in the cluster's primary zone (for zonal clusters)
or spread across three randomly chosen zones within the cluster's region (for
regional clusters).

Note that \f5NUM_NODES\fR nodes will be created in each zone, such that if you
specify \f5\-\-num\-nodes=4\fR and choose two locations, 8 nodes will be
created.

Multiple locations can be specified, separated by commas. For example:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-zone us\-central1\-a \e
    \-\-node\-locations us\-central1\-a,us\-central1\-b
.RE

.TP 2m
\fB\-\-node\-taints\fR=[\fINODE_TAINT\fR,...]
Applies the given kubernetes taints on all nodes in default node pool(s) in new
cluster, which can be used with tolerations for pod scheduling. Example:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-node\-taints=key1=val1:NoSchedule,key2=val2:PreferNoSchedule
.RE

Note, this feature uses \f5gcloud beta\fR commands. To use gcloud beta commands,
you must configure \f5gcloud\fR to use the v1beta1 API as described here:
https://cloud.google.com/kubernetes\-engine/docs/reference/api\-organization#beta.
To read more about node\-taints, see
https://cloud.google.com/kubernetes\-engine/docs/node\-taints.

.TP 2m
\fB\-\-node\-version\fR=\fINODE_VERSION\fR
The Kubernetes version to use for nodes. Defaults to server\-specified.

The default Kubernetes version is available using the following command.

.RS 2m
$ gcloud container get\-server\-config
.RE

.TP 2m
\fB\-\-num\-nodes\fR=\fINUM_NODES\fR; default=3
The number of nodes to be created in each of the cluster's zones.

.TP 2m
\fB\-\-preemptible\fR
Create nodes using preemptible VM instances in the new cluster.

.RS 2m
$ gcloud container clusters create example\-cluster \-\-preemptible
.RE

New nodes, including ones created by resize or recreate, will use preemptible VM
instances. See https://cloud.google.com/kubernetes\-engine/docs/preemptible\-vm
for more information on how to use Preemptible VMs with Kubernetes Engine.

.TP 2m
\fB\-\-release\-channel\fR=\fICHANNEL\fR
Release channel a cluster is subscribed to.

When a cluster is subscribed to a release channel, Google maintains both the
master version and the node version. Node auto\-upgrade defaults to true and
cannot be disabled.

\fICHANNEL\fR must be one of:

.RS 2m
.TP 2m
\fBrapid\fR
\'rapid' channel is offered on an early access basis for customers who want to
test new releases.

WARNING: Versions available in the 'rapid' channel may be subject to unresolved
issues with no known workaround and are not subject to any SLAs.

.TP 2m
\fBregular\fR
Clusters subscribed to 'regular' receive versions that are considered GA
quality. 'regular' is intended for production users who want to take advantage
of new features.

.TP 2m
\fBstable\fR
Clusters subscribed to 'stable' receive versions that are known to be stable and
reliable in production.

.RE
.sp


.TP 2m
\fB\-\-services\-ipv4\-cidr\fR=\fICIDR\fR
Set the IP range for the services IPs.

Can be specified as a netmask size (e.g. '/20') or as in CIDR notion (e.g.
\'10.100.0.0/20'). If given as a netmask size, the IP range will be chosen
automatically from the available space in the network.

If unspecified, the services CIDR range will be chosen with a default mask size.

Can not be specified unless '\-\-enable\-ip\-alias' is also specified.

.TP 2m
\fB\-\-services\-secondary\-range\-name\fR=\fINAME\fR
Set the secondary range to be used for services (e.g. ClusterIPs). NAME must be
the name of an existing secondary range in the cluster subnetwork.

Must be used in conjunction with '\-\-enable\-ip\-alias'. Cannot be used with
\-\-create\-subnetwork.

.TP 2m
\fB\-\-shielded\-integrity\-monitoring\fR
Enables monitoring and attestation of the boot integrity of the instance. The
attestation is performed against the integrity policy baseline. This baseline is
initially derived from the implicitly trusted boot image when the instance is
created.

.TP 2m
\fB\-\-shielded\-secure\-boot\fR
The instance will boot with secure boot enabled.

.TP 2m
\fB\-\-subnetwork\fR=\fISUBNETWORK\fR
The Google Compute Engine subnetwork
(https://cloud.google.com/compute/docs/subnetworks) to which the cluster is
connected. The subnetwork must belong to the network specified by \-\-network.

Cannot be used with the "\-\-create\-subnetwork" option.

.TP 2m
\fB\-\-tags\fR=\fITAG\fR,[\fITAG\fR,...]
Applies the given Compute Engine tags (comma separated) on all nodes in the new
node\-pool. Example:

.RS 2m
$ gcloud container clusters create example\-cluster \-\-tags=tag1,tag2
.RE

New nodes, including ones created by resize or recreate, will have these tags on
the Compute Engine API instance object and can be used in firewall rules. See
https://cloud.google.com/sdk/gcloud/reference/compute/firewall\-rules/create for
examples.

.TP 2m
\fB\-\-workload\-metadata\fR=\fIWORKLOAD_METADATA\fR
Type of metadata server available to pods running in the node pool.
\fIWORKLOAD_METADATA\fR must be one of:

.RS 2m
.TP 2m
\fBGCE_METADATA\fR
Pods running in this node pool have access to the node's underlying Compute
Engine Metadata Server.
.TP 2m
\fBGKE_METADATA\fR
Run the Kubernetes Engine Metadata Server on this node. The Kubernetes Engine
Metadata Server exposes a metadata API to workloads that is compatible with the
V1 Compute Metadata APIs exposed by the Compute Engine and App Engine Metadata
Servers. This feature can only be enabled if Workload Identity is enabled at the
cluster level.
.RE
.sp


.TP 2m
\fB\-\-workload\-pool\fR=\fIWORKLOAD_POOL\fR
Enable Workload Identity on the cluster.

When enabled, Kubernetes service accounts will be able to act as Cloud IAM
Service Accounts, through the provided workload pool.

Currently, the only accepted workload pool is the workload pool of the Cloud
project containing the cluster, \f5PROJECT_ID.svc.id.goog\fR.

For more information on Workload Identity, see

.RS 2m
https://cloud.google.com/kubernetes\-engine/docs/how\-to/workload\-identity
.RE

.TP 2m

Node autoprovisioning

.RS 2m
.TP 2m
\fB\-\-enable\-autoprovisioning\fR
Enables node autoprovisioning for a cluster.

Cluster Autoscaler will be able to create new node pools. Requires maximum CPU
and memory limits to be specified. This flag must be specified if any of the
other arguments in this group are specified.

.TP 2m

At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-autoprovisioning\-config\-file\fR=\fIAUTOPROVISIONING_CONFIG_FILE\fR
Path of the JSON/YAML file which contains information about the cluster's node
autoprovisioning configuration. Currently it contains a list of resource limits,
identity defaults for autoprovisioning, node upgrade settings, node management
settings, minimum cpu platform, node locations for autoprovisioning, disk type
and size configuration, shielded instance settings, and customer\-managed
encryption keys settings.

Resource limits are specified in the field 'resourceLimits'. Each resource
limits definition contains three fields: resourceType, maximum and minimum.
Resource type can be "cpu", "memory" or an accelerator (e.g.
"nvidia\-tesla\-k80" for nVidia Tesla K80). Use gcloud compute
accelerator\-types list to learn about available accelerator types. Maximum is
the maximum allowed amount with the unit of the resource. Minimum is the minimum
allowed amount with the unit of the resource.

Identity default contains at most one of the below fields: serviceAccount: The
Google Cloud Platform Service Account to be used by node VMs in autoprovisioned
node pools. If not specified, the project's default service account is used.
scopes: A list of scopes to be used by node instances in autoprovisioned node
pools. Multiple scopes can be specified, separated by commas. For information on
defaults, look at:
https://cloud.google.com/sdk/gcloud/reference/container/clusters/create#\-\-scopes

Node Upgrade settings are specified under the field 'upgradeSettings', which has
the following fields: maxSurgeUpgrade: Number of extra (surge) nodes to be
created on each upgrade of an autoprovisioned node pool. maxUnavailableUpgrade:
Number of nodes that can be unavailable at the same time on each upgrade of an
autoprovisioned node pool.

Node Management settings are specified under the field 'nodeManagement', which
has the following fields: enableAutoUpgrade: A boolean field that indicates if
node autoupgrade is enabled for autoprovisioned node pools. enableAutoRepair: A
boolean field that indicates if node autorepair is enabled for autoprovisioned
node pools.

minCpuPlatform: If specified, new autoprovisioned nodes will be scheduled on
host with specified CPU architecture or a newer one. Note: Min CPU platform can
only be specified in Beta and Alpha.

Autoprovisioning locations is a set of zones where new node pools can be created
by Autoprovisioning. Autoprovisioning locations are specified in the field
\'autoprovisioningLocations'. All zones must be in the same region as the
cluster's master(s).

Disk type and size are specified under the 'diskType' and 'diskSizeGb' fields,
respectively. If specified, new autoprovisioned nodes will be created with
custom boot disks configured by these settings.

Shielded instance settings are specified under the 'shieldedInstanceConfig'
field, which has the following fields: enableSecureBoot: A boolean field that
indicates if secure boot is enabled for autoprovisioned nodes.
enableIntegrityMonitoring: A boolean field that indicates if integrity
monitoring is enabled for autoprovisioned nodes.

Customer Managed Encryption Keys (CMEK) used by new auto\-provisioned node pools
can be specified in the 'bootDiskKmsKey' field.

.TP 2m

Flags to configure autoprovisioned nodes

.RS 2m
.TP 2m
\fB\-\-max\-cpu\fR=\fIMAX_CPU\fR
Maximum number of cores in the cluster.

Maximum number of cores to which the cluster can scale. This flag must be
specified if any of the other arguments in this group are specified.

.TP 2m
\fB\-\-max\-memory\fR=\fIMAX_MEMORY\fR
Maximum memory in the cluster.

Maximum number of gigabytes of memory to which the cluster can scale. This flag
must be specified if any of the other arguments in this group are specified.

.TP 2m
\fB\-\-autoprovisioning\-locations\fR=\fIZONE\fR,[\fIZONE\fR,...]
Set of zones where new node pools can be created by autoprovisioning. All zones
must be in the same region as the cluster's master(s). Multiple locations can be
specified, separated by commas.

.TP 2m
\fB\-\-autoprovisioning\-min\-cpu\-platform\fR=\fIPLATFORM\fR
If specified, new autoprovisioned nodes will be scheduled on host with specified
CPU architecture or a newer one.

.TP 2m
\fB\-\-min\-cpu\fR=\fIMIN_CPU\fR
Minimum number of cores in the cluster.

Minimum number of cores to which the cluster can scale.

.TP 2m
\fB\-\-min\-memory\fR=\fIMIN_MEMORY\fR
Minimum memory in the cluster.

Minimum number of gigabytes of memory to which the cluster can scale.

.TP 2m

Flags to specify upgrade settings for autoprovisioned nodes:

.RS 2m
.TP 2m
\fB\-\-autoprovisioning\-max\-surge\-upgrade\fR=\fIAUTOPROVISIONING_MAX_SURGE_UPGRADE\fR
Number of extra (surge) nodes to be created on each upgrade of an
autoprovisioned node pool. This flag must be specified if any of the other
arguments in this group are specified.

.TP 2m
\fB\-\-autoprovisioning\-max\-unavailable\-upgrade\fR=\fIAUTOPROVISIONING_MAX_UNAVAILABLE_UPGRADE\fR
Number of nodes that can be unavailable at the same time on each upgrade of an
autoprovisioned node pool. This flag must be specified if any of the other
arguments in this group are specified.

.RE
.sp
.TP 2m

Flags to specify identity for autoprovisioned nodes:

.RS 2m
.TP 2m
\fB\-\-autoprovisioning\-scopes\fR=[\fISCOPE\fR,...]
The scopes be used by node instances in autoprovisioned node pools. Multiple
scopes can be specified, separated by commas. For information on defaults, look
at:
https://cloud.google.com/sdk/gcloud/reference/container/clusters/create#\-\-scopes

.TP 2m
\fB\-\-autoprovisioning\-service\-account\fR=\fIAUTOPROVISIONING_SERVICE_ACCOUNT\fR
The Google Cloud Platform Service Account to be used by node VMs in
autoprovisioned node pools. If not specified, the project default service
account is used.

.RE
.sp
.TP 2m

Flags to specify node management settings for autoprovisioned nodes:

.RS 2m
.TP 2m
\fB\-\-enable\-autoprovisioning\-autorepair\fR
Enable node autorepair for autoprovisioned node pools. Use
\-\-no\-enable\-autoprovisioning\-autorepair to disable. This flag must be
specified if any of the other arguments in this group are specified.

.TP 2m
\fB\-\-enable\-autoprovisioning\-autoupgrade\fR
Enable node autoupgrade for autoprovisioned node pools. Use
\-\-no\-enable\-autoprovisioning\-autoupgrade to disable. This flag must be
specified if any of the other arguments in this group are specified.

.RE
.sp
.TP 2m

Arguments to set limits on accelerators:

.RS 2m
.TP 2m
\fB\-\-max\-accelerator\fR=[\fItype\fR=\fITYPE\fR,\fIcount\fR=\fICOUNT\fR,...]
Sets maximum limit for a single type of accelerators (e.g. GPUs) in cluster.

.RE
.RE
.RE
.sp
.TP 2m
\fBtype\fR
(Required) The specific type (e.g. nvidia\-tesla\-k80 for nVidia Tesla K80) of
accelerator for which the limit is set. Use \f5gcloud compute accelerator\-types
list\fR to learn about all available accelerator types.

.TP 2m
\fBcount\fR
(Required) The maximum number of accelerators to which the cluster can be
scaled. This flag must be specified if any of the other arguments in this group
are specified.

.RS 2m
.TP 2m
\fB\-\-min\-accelerator\fR=[\fItype\fR=\fITYPE\fR,\fIcount\fR=\fICOUNT\fR,...]
Sets minimum limit for a single type of accelerators (e.g. GPUs) in cluster.
Defaults to 0 for all accelerator types if it isn't set.

.RE
.sp
.TP 2m
\fBtype\fR
(Required) The specific type (e.g. nvidia\-tesla\-k80 for nVidia Tesla K80) of
accelerator for which the limit is set. Use \f5gcloud compute accelerator\-types
list\fR to learn about all available accelerator types.

.TP 2m
\fBcount\fR
(Required) The minimum number of accelerators to which the cluster can be
scaled.

.RE
.sp
.TP 2m

Cluster autoscaling

.RS 2m
.TP 2m
\fB\-\-enable\-autoscaling\fR
Enables autoscaling for a node pool.

Enables autoscaling in the node pool specified by \-\-node\-pool or the default
node pool if \-\-node\-pool is not provided.

.TP 2m
\fB\-\-max\-nodes\fR=\fIMAX_NODES\fR
Maximum number of nodes in the node pool.

Maximum number of nodes to which the node pool specified by \-\-node\-pool (or
default node pool if unspecified) can scale. Ignored unless
\-\-enable\-autoscaling is also specified.

.TP 2m
\fB\-\-min\-nodes\fR=\fIMIN_NODES\fR
Minimum number of nodes in the node pool.

Minimum number of nodes to which the node pool specified by \-\-node\-pool (or
default node pool if unspecified) can scale. Ignored unless
\-\-enable\-autoscaling is also specified.

.RE
.sp
.TP 2m

Master Authorized Networks

.RS 2m
.TP 2m
\fB\-\-enable\-master\-authorized\-networks\fR
Allow only specified set of CIDR blocks (specified by the
\f5\-\-master\-authorized\-networks\fR flag) to connect to Kubernetes master
through HTTPS. Besides these blocks, the following have access as well:

.RS 2m
1) The private network the cluster connects to if
`\-\-enable\-private\-nodes` is specified.
2) Google Compute Engine Public IPs if `\-\-enable\-private\-nodes` is not
specified.
.RE

Use \f5\-\-no\-enable\-master\-authorized\-networks\fR to disable. When
disabled, public internet (0.0.0.0/0) is allowed to connect to Kubernetes master
through HTTPS.

.TP 2m
\fB\-\-master\-authorized\-networks\fR=\fINETWORK\fR,[\fINETWORK\fR,...]
The list of CIDR blocks (up to 100 for private cluster, 50 for public cluster)
that are allowed to connect to Kubernetes master through HTTPS. Specified in
CIDR notation (e.g. 1.2.3.4/30). Cannot be specified unless
\f5\-\-enable\-master\-authorized\-networks\fR is also specified.

.RE
.sp
.TP 2m

Exports cluster's usage of cloud resources

.RS 2m
.TP 2m
\fB\-\-enable\-network\-egress\-metering\fR
Enable network egress metering on this cluster.

When enabled, a DaemonSet is deployed into the cluster. Each DaemonSet pod
meters network egress traffic by collecting data from the conntrack table, and
exports the metered metrics to the specified destination.

Network egress metering is disabled if this flag is omitted, or when
\f5\-\-no\-enable\-network\-egress\-metering\fR is set.

.TP 2m
\fB\-\-enable\-resource\-consumption\-metering\fR
Enable resource consumption metering on this cluster.

When enabled, a table will be created in the specified BigQuery dataset to store
resource consumption data. The resulting table can be joined with the resource
usage table or with BigQuery billing export.

Resource consumption metering is enabled unless \f5\-\-no\-enable\-resource\-
consumption\-metering\fR is set.

.TP 2m
\fB\-\-resource\-usage\-bigquery\-dataset\fR=\fIRESOURCE_USAGE_BIGQUERY_DATASET\fR
The name of the BigQuery dataset to which the cluster's usage of cloud resources
is exported. A table will be created in the specified dataset to store cluster
resource usage. The resulting table can be joined with BigQuery Billing Export
to produce a fine\-grained cost breakdown.

Example:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-resource\-usage\-bigquery\-dataset=example_bigquery_dataset_name
.RE

.RE
.sp
.TP 2m

Private Clusters

.RS 2m
.TP 2m
\fB\-\-enable\-private\-endpoint\fR
Cluster is managed using the private IP address of the master API endpoint.

.TP 2m
\fB\-\-enable\-private\-nodes\fR
Cluster is created with no public IP addresses on the cluster nodes.

.TP 2m
\fB\-\-master\-ipv4\-cidr\fR=\fIMASTER_IPV4_CIDR\fR
IPv4 CIDR range to use for the master network. This should have a netmask of
size /28 and should be used in conjunction with the \-\-enable\-private\-nodes
flag.

.RE
.sp
.TP 2m

Flags relating to Cloud TPUs:

.RS 2m
.TP 2m
\fB\-\-enable\-tpu\fR
Enable Cloud TPUs for this cluster.

Can not be specified unless \f5\-\-enable\-ip\-alias\fR is also specified.

.TP 2m
\fB\-\-tpu\-ipv4\-cidr\fR=\fICIDR\fR
Set the IP range for the Cloud TPUs.

Can be specified as a netmask size (e.g. '/20') or as in CIDR notion (e.g.
\'10.100.0.0/20'). If given as a netmask size, the IP range will be chosen
automatically from the available space in the network.

If unspecified, the TPU CIDR range will use automatic default '/20'.

Can not be specified unless '\-\-enable\-tpu' and '\-\-enable\-ip\-alias' are
also specified.

.RE
.sp
.TP 2m

One of either maintenance\-window or the group of maintenance\-window flags can
be set. At most one of these may be specified:


.RS 2m
.TP 2m
\fB\-\-maintenance\-window\fR=\fISTART_TIME\fR
Set a time of day when you prefer maintenance to start on this cluster. For
example:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-maintenance\-window=12:43
.RE

The time corresponds to the UTC time zone, and must be in HH:MM format.

Non\-emergency maintenance will occur in the 4 hour block starting at the
specified time.

This is mutually exclusive with the recurring maintenance windows and will
overwrite any existing window. Compatible with maintenance exclusions.

.TP 2m

Set a flexible maintenance window by specifying a window that recurs per an RFC
5545 RRULE. Non\-emergency maintenance will occur in the recurring windows.

Examples:

For a 9\-5 Mon\-Wed UTC\-4 maintenance window:

.RS 2m
$ gcloud container clusters create example\-cluster   \e
    \-\-maintenance\-window\-start=2000\-01\-01T09:00:00\-04:00   \e
    \-\-maintenance\-window\-end=2000\-01\-01T17:00:00\-04:00   \e
    \-\-maintenance\-window\-recurrence='FREQ=WEEKLY;BYDAY=MO,TU,WE'
.RE

For a daily window from 22:00 \- 04:00 UTC:

.RS 2m
$ gcloud container clusters create example\-cluster   \e
    \-\-maintenance\-window\-start=2000\-01\-01T22:00:00Z   \e
    \-\-maintenance\-window\-end=2000\-01\-02T04:00:00Z   \e
    \-\-maintenance\-window\-recurrence=FREQ=DAILY
.RE



.RS 2m
.TP 2m
\fB\-\-maintenance\-window\-end\fR=\fITIME_STAMP\fR
End time of the first window (can occur in the past). Must take place after the
start time. The difference in start and end time specifies the length of each
recurrence. See $ gcloud topic datetimes for information on time formats. This
flag must be specified if any of the other arguments in this group are
specified.

.TP 2m
\fB\-\-maintenance\-window\-recurrence\fR=\fIRRULE\fR
An RFC 5545 RRULE, specifying how the window will recur. Note that minimum
requirements for maintenance periods will be enforced. Note that FREQ=SECONDLY,
MINUTELY, and HOURLY are not supported. This flag must be specified if any of
the other arguments in this group are specified.

.TP 2m
\fB\-\-maintenance\-window\-start\fR=\fITIME_STAMP\fR
Start time of the first window (can occur in the past). The start time
influences when the window will start for recurrences. See $ gcloud topic
datetimes for information on time formats. This flag must be specified if any of
the other arguments in this group are specified.

.RE
.RE
.sp
.TP 2m

Basic auth

.RS 2m
.TP 2m
\fB\-\-password\fR=\fIPASSWORD\fR
The password to use for cluster auth. Defaults to a server\-specified
randomly\-generated string.

.TP 2m

Options to specify the username. At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-enable\-basic\-auth\fR
Enable basic (username/password) auth for the cluster.
\f5\-\-enable\-basic\-auth\fR is an alias for \f5\-\-username=admin\fR;
\f5\-\-no\-enable\-basic\-auth\fR is an alias for \f5\-\-username=""\fR. Use
\f5\-\-password\fR to specify a password; if not, the server will randomly
generate one. For cluster versions before 1.12, if neither
\f5\-\-enable\-basic\-auth\fR nor \f5\-\-username\fR is specified,
\f5\-\-enable\-basic\-auth\fR will default to \f5true\fR. After 1.12,
\f5\-\-enable\-basic\-auth\fR will default to \f5false\fR.

.TP 2m
\fB\-\-username\fR=\fIUSERNAME\fR, \fB\-u\fR \fIUSERNAME\fR
The user name to use for basic auth for the cluster. Use \f5\-\-password\fR to
specify a password; if not, the server will randomly generate one.

.RE
.RE
.sp
.TP 2m

At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-region\fR=\fIREGION\fR
Compute region (e.g. us\-central1) for the cluster.

.TP 2m
\fB\-\-zone\fR=\fIZONE\fR, \fB\-z\fR \fIZONE\fR
Compute zone (e.g. us\-central1\-a) for the cluster. Overrides the default
\fBcompute/zone\fR property value for this command invocation.

.RE
.sp
.TP 2m

Specifies the reservation for the default initial node pool.

.RS 2m
.TP 2m
\fB\-\-reservation\fR=\fIRESERVATION\fR
The name of the reservation, required when
\f5\-\-reservation\-affinity=specific\fR.

.TP 2m
\fB\-\-reservation\-affinity\fR=\fIRESERVATION_AFFINITY\fR
The type of the reservation for the default initial node pool.
\fIRESERVATION_AFFINITY\fR must be one of: \fBany\fR, \fBnone\fR,
\fBspecific\fR.

.RE
.sp
.TP 2m

Options to specify the node identity.

.RS 2m
.TP 2m

Scopes options.

.RS 2m
.TP 2m
\fB\-\-scopes\fR=[\fISCOPE\fR,...]; default="gke\-default"
Specifies scopes for the node instances. Examples:

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-scopes=https://www.googleapis.com/auth/devstorage.read_only
.RE

.RS 2m
$ gcloud container clusters create example\-cluster \e
    \-\-scopes=bigquery,storage\-rw,compute\-ro
.RE

Multiple SCOPEs can be specified, separated by commas. \f5logging\-write\fR
and/or \f5monitoring\fR are added unless Cloud Logging and/or Cloud Monitoring
are disabled (see \f5\-\-enable\-cloud\-logging\fR and
\f5\-\-enable\-cloud\-monitoring\fR for more information). SCOPE can be either
the full URI of the scope or an alias. \fBdefault\fR scopes are assigned to all
instances. Available aliases are:


.TS
tab(	);
lB lB
l l.
Alias	URI
bigquery	https://www.googleapis.com/auth/bigquery
cloud-platform	https://www.googleapis.com/auth/cloud-platform
cloud-source-repos	https://www.googleapis.com/auth/source.full_control
cloud-source-repos-ro	https://www.googleapis.com/auth/source.read_only
compute-ro	https://www.googleapis.com/auth/compute.readonly
compute-rw	https://www.googleapis.com/auth/compute
datastore	https://www.googleapis.com/auth/datastore
default	https://www.googleapis.com/auth/devstorage.read_only
	https://www.googleapis.com/auth/logging.write
	https://www.googleapis.com/auth/monitoring.write
	https://www.googleapis.com/auth/pubsub
	https://www.googleapis.com/auth/service.management.readonly
	https://www.googleapis.com/auth/servicecontrol
	https://www.googleapis.com/auth/trace.append
gke-default	https://www.googleapis.com/auth/devstorage.read_only
	https://www.googleapis.com/auth/logging.write
	https://www.googleapis.com/auth/monitoring
	https://www.googleapis.com/auth/service.management.readonly
	https://www.googleapis.com/auth/servicecontrol
	https://www.googleapis.com/auth/trace.append
logging-write	https://www.googleapis.com/auth/logging.write
monitoring	https://www.googleapis.com/auth/monitoring
monitoring-read	https://www.googleapis.com/auth/monitoring.read
monitoring-write	https://www.googleapis.com/auth/monitoring.write
pubsub	https://www.googleapis.com/auth/pubsub
service-control	https://www.googleapis.com/auth/servicecontrol
service-management	https://www.googleapis.com/auth/service.management.readonly
sql (deprecated)	https://www.googleapis.com/auth/sqlservice
sql-admin	https://www.googleapis.com/auth/sqlservice.admin
storage-full	https://www.googleapis.com/auth/devstorage.full_control
storage-ro	https://www.googleapis.com/auth/devstorage.read_only
storage-rw	https://www.googleapis.com/auth/devstorage.read_write
taskqueue	https://www.googleapis.com/auth/taskqueue
trace	https://www.googleapis.com/auth/trace.append
userinfo-email	https://www.googleapis.com/auth/userinfo.email
.TE

DEPRECATION WARNING: https://www.googleapis.com/auth/sqlservice account scope
and \f5sql\fR alias do not provide SQL instance management capabilities and have
been deprecated. Please, use https://www.googleapis.com/auth/sqlservice.admin or
\f5sql\-admin\fR to manage your Google SQL Service instances.


.RE
.sp
.TP 2m
\fB\-\-service\-account\fR=\fISERVICE_ACCOUNT\fR
The Google Cloud Platform Service Account to be used by the node VMs. If a
service account is specified, the cloud\-platform and userinfo.email scopes are
used. If no Service Account is specified, the project default service account is
used.


.RE
.RE
.sp

.SH "GCLOUD WIDE FLAGS"

These flags are available to all commands: \-\-account, \-\-billing\-project,
\-\-configuration, \-\-flags\-file, \-\-flatten, \-\-format, \-\-help,
\-\-impersonate\-service\-account, \-\-log\-http, \-\-project, \-\-quiet,
\-\-trace\-token, \-\-user\-output\-enabled, \-\-verbosity.

Run \fB$ gcloud help\fR for details.



.SH "EXAMPLES"

To create a cluster with the default configuration, run:

.RS 2m
$ gcloud container clusters create sample\-cluster
.RE



.SH "NOTES"

These variants are also available:

.RS 2m
$ gcloud alpha container clusters create
$ gcloud beta container clusters create
.RE

