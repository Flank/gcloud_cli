
.TH "GCLOUD_ALPHA_ML\-ENGINE_VERSIONS_CREATE" 1



.SH "NAME"
.HP
gcloud alpha ml\-engine versions create \- create a new AI Platform version



.SH "SYNOPSIS"
.HP
\f5gcloud alpha ml\-engine versions create\fR \fIVERSION\fR \fB\-\-model\fR=\fIMODEL\fR [\fB\-\-accelerator\fR=[\fIcount\fR=\fICOUNT\fR],[\fItype\fR=\fITYPE\fR]] [\fB\-\-async\fR] [\fB\-\-config\fR=\fICONFIG\fR] [\fB\-\-description\fR=\fIDESCRIPTION\fR] [\fB\-\-explanation\-method\fR=\fIEXPLANATION_METHOD\fR] [\fB\-\-framework\fR=\fIFRAMEWORK\fR] [\fB\-\-labels\fR=[\fIKEY\fR=\fIVALUE\fR,...]] [\fB\-\-machine\-type\fR=\fIMACHINE_TYPE\fR] [\fB\-\-num\-integral\-steps\fR=\fINUM_INTEGRAL_STEPS\fR;\ default=50] [\fB\-\-num\-paths\fR=\fINUM_PATHS\fR;\ default=50] [\fB\-\-origin\fR=\fIORIGIN\fR] [\fB\-\-python\-version\fR=\fIPYTHON_VERSION\fR] [\fB\-\-region\fR=\fIREGION\fR] [\fB\-\-runtime\-version\fR=\fIRUNTIME_VERSION\fR] [\fB\-\-service\-account\fR=\fISERVICE_ACCOUNT\fR] [\fB\-\-staging\-bucket\fR=\fISTAGING_BUCKET\fR] [\fB\-\-args\fR=[\fIARG\fR,...]\ \fB\-\-command\fR=[\fICOMMAND\fR,...]\ \fB\-\-env\-vars\fR=[\fIKEY\fR=\fIVALUE\fR,...]\ \fB\-\-image\fR=\fIIMAGE\fR\ \fB\-\-ports\fR=[\fIARG\fR,...]] [\fB\-\-health\-route\fR=\fIHEALTH_ROUTE\fR\ \fB\-\-predict\-route\fR=\fIPREDICT_ROUTE\fR] [\fB\-\-package\-uris\fR=[\fIPACKAGE_URI\fR,...]\ \fB\-\-prediction\-class\fR=\fIPREDICTION_CLASS\fR] [\fIGCLOUD_WIDE_FLAG\ ...\fR]



.SH "DESCRIPTION"

\fB(ALPHA)\fR Creates a new version of an AI Platform model.

For more details on managing AI Platform models and versions see
https://cloud.google.com/ai\-platform/prediction/docs/managing\-models\-jobs



.SH "POSITIONAL ARGUMENTS"

.RS 2m
.TP 2m
\fIVERSION\fR
Name of the model version.


.RE
.sp

.SH "REQUIRED FLAGS"

.RS 2m
.TP 2m
\fB\-\-model\fR=\fIMODEL\fR
Name of the model.


.RE
.sp

.SH "OPTIONAL FLAGS"

.RS 2m
.TP 2m
\fB\-\-accelerator\fR=[\fIcount\fR=\fICOUNT\fR],[\fItype\fR=\fITYPE\fR]
Manage the accelerator config for GPU serving. When deploying a model with
Compute Engine Machine Types, a GPU accelerator may also be selected.

.RS 2m
.TP 2m
\fBtype\fR
The type of the accelerator. Choices are 'nvidia\-tesla\-a100',
\'nvidia\-tesla\-k80', 'nvidia\-tesla\-p100', 'nvidia\-tesla\-p4',
\'nvidia\-tesla\-t4', 'nvidia\-tesla\-v100'.

.TP 2m
\fBcount\fR
The number of accelerators to attach to each machine running the job. This is
usually 1; scaling parameters are configured using \f5manualScaling\fR or
\f5autoScaling\fR flags.

.RE
.sp
.TP 2m
\fB\-\-async\fR
Return immediately, without waiting for the operation in progress to complete.

.TP 2m
\fB\-\-config\fR=\fICONFIG\fR
Path to a YAML configuration file containing configuration parameters for the
Version
(https://cloud.google.com/ai\-platform/prediction/docs/reference/rest/v1/projects.models.versions)
to create.

The file is in YAML format. Note that not all attributes of a version are
configurable; available attributes (with example values) are:

.RS 2m
description: A free\-form description of the version.
deploymentUri: gs://path/to/source
runtimeVersion: '2.1'
#  Set only one of either manualScaling or autoScaling.
manualScaling:
  nodes: 10  # The number of nodes to allocate for this model.
autoScaling:
  minNodes: 0  # The minimum number of nodes to allocate for this model.
labels:
  user\-defined\-key: user\-defined\-value
.RE

The name of the version must always be specified via the required VERSION
argument.

Only one of manualScaling or autoScaling can be specified. If both are specified
in same yaml file an error will be returned.

If an option is specified both in the configuration file and via command\-line
arguments, the command\-line arguments override the configuration file.

.TP 2m
\fB\-\-description\fR=\fIDESCRIPTION\fR
The description of the version.

.TP 2m
\fB\-\-explanation\-method\fR=\fIEXPLANATION_METHOD\fR
Enable explanations and select the explanation method to use.

The valid options are: integrated\-gradients: Use Integrated Gradients.
sampled\-shapley: Use Sampled Shapley. xrai: Use XRAI.

\fIEXPLANATION_METHOD\fR must be one of: \fBintegrated\-gradients\fR,
\fBsampled\-shapley\fR, \fBxrai\fR.

.TP 2m
\fB\-\-framework\fR=\fIFRAMEWORK\fR
The ML framework used to train this version of the model. If not specified,
defaults to 'tensorflow'. \fIFRAMEWORK\fR must be one of: \fBscikit\-learn\fR,
\fBtensorflow\fR, \fBxgboost\fR.

.TP 2m
\fB\-\-labels\fR=[\fIKEY\fR=\fIVALUE\fR,...]
List of label KEY=VALUE pairs to add.

Keys must start with a lowercase character and contain only hyphens (\f5\-\fR),
underscores (\f5_\fR), lowercase characters, and numbers. Values must contain
only hyphens (\f5\-\fR), underscores (\f5_\fR), lowercase characters, and
numbers.

.TP 2m
\fB\-\-machine\-type\fR=\fIMACHINE_TYPE\fR
Type of machine on which to serve the model. Currently only applies to online
prediction. For available machine types, see
https://cloud.google.com/ai\-platform/prediction/docs/machine\-types\-online\-prediction#available_machine_types.

.TP 2m
\fB\-\-num\-integral\-steps\fR=\fINUM_INTEGRAL_STEPS\fR; default=50
Number of integral steps for Integrated Gradients. Only valid when
\f5\-\-explanation\-method=integrated\-gradients\fR or
\f5\-\-explanation\-method=xrai\fR is specified.

.TP 2m
\fB\-\-num\-paths\fR=\fINUM_PATHS\fR; default=50
Number of paths for Sampled Shapley. Only valid when
\f5\-\-explanation\-method=sampled\-shapley\fR is specified.

.TP 2m
\fB\-\-origin\fR=\fIORIGIN\fR
Location of \f5model/\fR "directory" (see
https://cloud.google.com/ai\-platform/prediction/docs/deploying\-models#upload\-model).

This overrides \f5deploymentUri\fR in the \f5\-\-config\fR file. If this flag is
not passed, \f5deploymentUri\fR \fBmust\fR be specified in the file from
\f5\-\-config\fR.

Can be a Cloud Storage (\f5gs://\fR) path or local file path (no prefix). In the
latter case the files will be uploaded to Cloud Storage and a
\f5\-\-staging\-bucket\fR argument is required.

.TP 2m
\fB\-\-python\-version\fR=\fIPYTHON_VERSION\fR
Version of Python used when creating the version. Choices are 3.7, 3.5, and 2.7.
However, this value must be compatible with the chosen runtime version for the
job.

Must be used with a compatible runtime version:

.RS 2m
.IP "\(em" 2m
3.7 is compatible with runtime versions 1.15 and later.
.IP "\(em" 2m
3.5 is compatible with runtime versions 1.4 through 1.14.
.IP "\(em" 2m
2.7 is compatible with runtime versions 1.15 and earlier.
.RE
.RE
.sp

.RS 2m
.TP 2m
\fB\-\-region\fR=\fIREGION\fR
Google Cloud region of the regional endpoint to use for this command. If
unspecified, the command uses the global endpoint of the AI Platform Training
and Prediction API.

Learn more about regional endpoints and see a list of available regions:
https://cloud.google.com/ai\-platform/prediction/docs/regional\-endpoints

\fIREGION\fR must be one of: \fBasia\-east1\fR, \fBeurope\-west4\fR,
\fBus\-central1\fR.

.TP 2m
\fB\-\-runtime\-version\fR=\fIRUNTIME_VERSION\fR
AI Platform runtime version for this job. Must be specified unless
\-\-master\-image\-uri is specified instead. It is defined in documentation
along with the list of supported versions:
https://cloud.google.com/ai\-platform/prediction/docs/runtime\-version\-list

.TP 2m
\fB\-\-service\-account\fR=\fISERVICE_ACCOUNT\fR
Specifies the service account for resource access control.

.TP 2m
\fB\-\-staging\-bucket\fR=\fISTAGING_BUCKET\fR
Bucket in which to stage training archives.

Required only if a file upload is necessary (that is, other flags include local
paths) and no other flags implicitly specify an upload path.

.TP 2m

Configure the container to be deployed.

.RS 2m
.TP 2m
\fB\-\-args\fR=[\fIARG\fR,...]
Comma\-separated arguments passed to the command run by the container image. If
not specified and no '\-\-command' is provided, the container image's default
Cmd is used.

.TP 2m
\fB\-\-command\fR=[\fICOMMAND\fR,...]
Entrypoint for the container image. If not specified, the container image's
default Entrypoint is run.

.TP 2m
\fB\-\-env\-vars\fR=[\fIKEY\fR=\fIVALUE\fR,...]
List of key\-value pairs to set as environment variables.

.TP 2m
\fB\-\-image\fR=\fIIMAGE\fR
Name of the container image to deploy (e.g. gcr.io/myproject/server:latest).

.TP 2m
\fB\-\-ports\fR=[\fIARG\fR,...]
Container ports to receive requests at. Must be a number between 1 and 65535,
inclusive.

.RE
.sp
.TP 2m

Flags to control the paths that requests and health checks are sent to.

.RS 2m
.TP 2m
\fB\-\-health\-route\fR=\fIHEALTH_ROUTE\fR
HTTP path to send health checks to inside the container.

.TP 2m
\fB\-\-predict\-route\fR=\fIPREDICT_ROUTE\fR
HTTP path to send prediction requests to inside the container.

.RE
.sp
.TP 2m

Configure user code in prediction. AI Platform allows a model to have
user\-provided prediction code; these options configure that code.



.RS 2m
.TP 2m
\fB\-\-package\-uris\fR=[\fIPACKAGE_URI\fR,...]
Comma\-separated list of Cloud Storage URIs ('gs://...') for user\-supplied
Python packages to use.

.TP 2m
\fB\-\-prediction\-class\fR=\fIPREDICTION_CLASS\fR
The fully\-qualified name of the custom prediction class in the package provided
for custom prediction.

For example, \f5\-\-prediction\-class=my_package.SequenceModel\fR.


.RE
.RE
.sp

.SH "GCLOUD WIDE FLAGS"

These flags are available to all commands: \-\-account, \-\-billing\-project,
\-\-configuration, \-\-flags\-file, \-\-flatten, \-\-format, \-\-help,
\-\-impersonate\-service\-account, \-\-log\-http, \-\-project, \-\-quiet,
\-\-trace\-token, \-\-user\-output\-enabled, \-\-verbosity.

Run \fB$ gcloud help\fR for details.



.SH "EXAMPLES"

To create an AI Platform version model with the version ID 'versionId' and with
the name 'model\-name', run:

.RS 2m
$ gcloud alpha ml\-engine versions create versionId \-\-model=model\-name
.RE



.SH "NOTES"

This command is currently in ALPHA and may change without notice. If this
command fails with API permission errors despite specifying the right project,
you may be trying to access an API with an invitation\-only early access
allowlist. These variants are also available:

.RS 2m
$ gcloud ml\-engine versions create
$ gcloud beta ml\-engine versions create
.RE

