
.TH "GCLOUD_ML\-ENGINE_PREDICT" 1



.SH "NAME"
.HP
gcloud ml\-engine predict \- run AI Platform online prediction



.SH "SYNOPSIS"
.HP
\f5gcloud ml\-engine predict\fR \fB\-\-model\fR=\fIMODEL\fR (\fB\-\-json\-instances\fR=\fIJSON_INSTANCES\fR\ |\ \fB\-\-text\-instances\fR=\fITEXT_INSTANCES\fR) [\fB\-\-signature\-name\fR=\fISIGNATURE_NAME\fR] [\fB\-\-version\fR=\fIVERSION\fR] [\fIGCLOUD_WIDE_FLAG\ ...\fR]



.SH "DESCRIPTION"

\f5gcloud ml\-engine predict\fR sends a prediction request to AI Platform for
the given instances. This command will read up to 100 instances, though the
service itself will accept instances up to the payload limit size (currently,
1.5MB). If you are predicting on more instances, you should use batch prediction
via

.RS 2m
$ gcloud ml\-engine jobs submit prediction.
.RE



.SH "REQUIRED FLAGS"

.RS 2m
.TP 2m
\fB\-\-model\fR=\fIMODEL\fR
Name of the model.

.TP 2m

Exactly one of these must be specified:

.RS 2m
.TP 2m
\fB\-\-json\-instances\fR=\fIJSON_INSTANCES\fR
Path to a local file from which instances are read. Instances are in JSON
format; newline delimited.

An example of the JSON instances file:

.RS 2m
{"images": [0.0, ..., 0.1], "key": 3}
{"images": [0.0, ..., 0.1], "key": 2}
...
.RE

This flag accepts "\-" for stdin.

.TP 2m
\fB\-\-text\-instances\fR=\fITEXT_INSTANCES\fR
Path to a local file from which instances are read. Instances are in UTF\-8
encoded text format; newline delimited.

An example of the text instances file:

.RS 2m
107,4.9,2.5,4.5,1.7
100,5.7,2.8,4.1,1.3
...
.RE

This flag accepts "\-" for stdin.


.RE
.RE
.sp

.SH "OPTIONAL FLAGS"

.RS 2m
.TP 2m
\fB\-\-signature\-name\fR=\fISIGNATURE_NAME\fR
The name of the signature defined in the SavedModel to use for this job.
Defaults to DEFAULT_SERVING_SIGNATURE_DEF_KEY in
https://www.tensorflow.org/api_docs/python/tf/saved_model/signature_constants,
which is "serving_default". Only applies to TensorFlow models.

.TP 2m
\fB\-\-version\fR=\fIVERSION\fR
Model version to be used.

If unspecified, the default version of the model will be used. To list model
versions run

.RS 2m
$ gcloud ml\-engine versions list
.RE


.RE
.sp

.SH "GCLOUD WIDE FLAGS"

These flags are available to all commands: \-\-account, \-\-billing\-project,
\-\-configuration, \-\-flags\-file, \-\-flatten, \-\-format, \-\-help,
\-\-impersonate\-service\-account, \-\-log\-http, \-\-project, \-\-quiet,
\-\-trace\-token, \-\-user\-output\-enabled, \-\-verbosity.

Run \fB$ gcloud help\fR for details.



.SH "NOTES"

These variants are also available:

.RS 2m
$ gcloud alpha ml\-engine predict
$ gcloud beta ml\-engine predict
.RE

