
.TH "GCLOUD_DATAFLOW_SQL_QUERY" 1



.SH "NAME"
.HP
gcloud dataflow sql query \- execute the user\-specified SQL query on Dataflow



.SH "SYNOPSIS"
.HP
\f5gcloud dataflow sql query\fR \fIQUERY\fR \fB\-\-job\-name\fR=\fIJOB_NAME\fR \fB\-\-region\fR=\fIREGION\fR ([\fB\-\-bigquery\-table\fR=\fIBIGQUERY_TABLE\fR\ :\ \fB\-\-bigquery\-dataset\fR=\fIBIGQUERY_DATASET\fR\ \fB\-\-bigquery\-project\fR=\fIBIGQUERY_PROJECT\fR]\ [\fB\-\-pubsub\-topic\fR=\fIPUBSUB_TOPIC\fR\ :\ \fB\-\-pubsub\-project\fR=\fIPUBSUB_PROJECT\fR]) [\fB\-\-bigquery\-write\-disposition\fR=\fIBIGQUERY_WRITE_DISPOSITION\fR;\ default="write\-empty"] [\fB\-\-dataflow\-kms\-key\fR=\fIDATAFLOW_KMS_KEY\fR] [\fB\-\-disable\-public\-ips\fR] [\fB\-\-dry\-run\fR] [\fB\-\-max\-workers\fR=\fIMAX_WORKERS\fR] [\fB\-\-network\fR=\fINETWORK\fR] [\fB\-\-num\-workers\fR=\fINUM_WORKERS\fR] [\fB\-\-pubsub\-create\-disposition\fR=\fIPUBSUB_CREATE_DISPOSITION\fR;\ default="create\-if\-not\-found"] [\fB\-\-service\-account\-email\fR=\fISERVICE_ACCOUNT_EMAIL\fR] [\fB\-\-subnetwork\fR=\fISUBNETWORK\fR] [\fB\-\-worker\-machine\-type\fR=\fIWORKER_MACHINE_TYPE\fR] [\fB\-\-parameter\fR=\fIPARAMETER\fR\ |\ \fB\-\-parameters\-file\fR=\fIPARAMETERS_FILE\fR] [\fB\-\-worker\-region\fR=\fIWORKER_REGION\fR\ |\ \fB\-\-worker\-zone\fR=\fIWORKER_ZONE\fR\ |\ \fB\-\-zone\fR=\fIZONE\fR] [\fIGCLOUD_WIDE_FLAG\ ...\fR]



.SH "DESCRIPTION"

Execute the user\-specified SQL query on Dataflow. Queries must comply to the
ZetaSQL dialect (https://github.com/google/zetasql). Results may be written to
either BigQuery or Cloud Pub/Sub.



.SH "POSITIONAL ARGUMENTS"

.RS 2m
.TP 2m
\fIQUERY\fR
The SQL query to execute.


.RE
.sp

.SH "REQUIRED FLAGS"

.RS 2m
.TP 2m
\fB\-\-job\-name\fR=\fIJOB_NAME\fR
The unique name to assign to the Cloud Dataflow job.

.TP 2m
\fB\-\-region\fR=\fIREGION\fR
The region ID of the job's regional endpoint. Defaults to 'us\-central1'.

.TP 2m

The destination(s) for the output of the query. At least one of these must be
specified:

.RS 2m
.TP 2m

BigQuery table resource \- The BigQuery table to write query output to. The
arguments in this group can be used to specify the attributes of this resource.

.RS 2m
.TP 2m
\fB\-\-bigquery\-table\fR=\fIBIGQUERY_TABLE\fR
ID of the BigQuery table or fully qualified identifier for the BigQuery table.
This flag must be specified if any of the other arguments in this group are
specified.

.TP 2m
\fB\-\-bigquery\-dataset\fR=\fIBIGQUERY_DATASET\fR
The BigQuery dataset ID.

.TP 2m
\fB\-\-bigquery\-project\fR=\fIBIGQUERY_PROJECT\fR
The BigQuery project ID.

.RE
.sp
.TP 2m

Pub/Sub topic resource \- The Cloud Pub/Sub topic to write query output to. The
arguments in this group can be used to specify the attributes of this resource.

.RS 2m
.TP 2m
\fB\-\-pubsub\-topic\fR=\fIPUBSUB_TOPIC\fR
ID of the Pub/Sub topic or fully qualified identifier for the Pub/Sub topic.
This flag must be specified if any of the other arguments in this group are
specified.

.TP 2m
\fB\-\-pubsub\-project\fR=\fIPUBSUB_PROJECT\fR
The Pub/Sub project ID.


.RE
.RE
.RE
.sp

.SH "OPTIONAL FLAGS"

.RS 2m
.TP 2m
\fB\-\-bigquery\-write\-disposition\fR=\fIBIGQUERY_WRITE_DISPOSITION\fR; default="write\-empty"
The behavior of the BigQuery write operation. \fIBIGQUERY_WRITE_DISPOSITION\fR
must be one of: \fBwrite\-empty\fR, \fBwrite\-truncate\fR, \fBwrite\-append\fR.

.TP 2m
\fB\-\-dataflow\-kms\-key\fR=\fIDATAFLOW_KMS_KEY\fR
The Cloud KMS key to protect the job resources.

.TP 2m
\fB\-\-disable\-public\-ips\fR
The Cloud Dataflow workers must not use public IP addresses. Overrides the
default \fBdataflow/disable_public_ips\fR property value for this command
invocation.

.TP 2m
\fB\-\-dry\-run\fR
Construct but do not run the SQL pipeline, for smoke testing.

.TP 2m
\fB\-\-max\-workers\fR=\fIMAX_WORKERS\fR
The maximum number of workers to run.

.TP 2m
\fB\-\-network\fR=\fINETWORK\fR
The Compute Engine network for launching instances to run your pipeline.

.TP 2m
\fB\-\-num\-workers\fR=\fINUM_WORKERS\fR
The initial number of workers to use.

.TP 2m
\fB\-\-pubsub\-create\-disposition\fR=\fIPUBSUB_CREATE_DISPOSITION\fR; default="create\-if\-not\-found"
The behavior of the Pub/Sub create operation. \fIPUBSUB_CREATE_DISPOSITION\fR
must be one of: \fBcreate\-if\-not\-found\fR, \fBfail\-if\-not\-found\fR.

.TP 2m
\fB\-\-service\-account\-email\fR=\fISERVICE_ACCOUNT_EMAIL\fR
The service account to run the workers as.

.TP 2m
\fB\-\-subnetwork\fR=\fISUBNETWORK\fR
The Compute Engine subnetwork for launching instances to run your pipeline.

.TP 2m
\fB\-\-worker\-machine\-type\fR=\fIWORKER_MACHINE_TYPE\fR
The type of machine to use for workers. Defaults to server\-specified.

.TP 2m

At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-parameter\fR=\fIPARAMETER\fR
Parameters to pass to a query. Parameters must use the format name:type:value,
for example min_word_count:INT64:250.

.TP 2m
\fB\-\-parameters\-file\fR=\fIPARAMETERS_FILE\fR
Path to a file containing query parameters in JSON format. e.g.
[{"parameterType": {"type": "STRING"}, "parameterValue": {"value": "foo"},
"name": "x"}, {"parameterType": {"type": "FLOAT64"}, "parameterValue": {"value":
"1.0"}, "name": "y"}]

.RE
.sp
.TP 2m

Worker location options. At most one of these may be specified:

.RS 2m
.TP 2m
\fB\-\-worker\-region\fR=\fIWORKER_REGION\fR
The region to run the workers in.

.TP 2m
\fB\-\-worker\-zone\fR=\fIWORKER_ZONE\fR
The zone to run the workers in.

.TP 2m
\fB\-\-zone\fR=\fIZONE\fR
(DEPRECATED) The zone to run the workers in.

The \-\-zone option is deprecated; use \-\-worker\-region or \-\-worker\-zone
instead.


.RE
.RE
.sp

.SH "GCLOUD WIDE FLAGS"

These flags are available to all commands: \-\-account, \-\-billing\-project,
\-\-configuration, \-\-flags\-file, \-\-flatten, \-\-format, \-\-help,
\-\-impersonate\-service\-account, \-\-log\-http, \-\-project, \-\-quiet,
\-\-trace\-token, \-\-user\-output\-enabled, \-\-verbosity.

Run \fB$ gcloud help\fR for details.



.SH "EXAMPLES"

To execute a simple SQL query on Dataflow that reads from and writes to
BigQuery, run:

.RS 2m
$ gcloud dataflow sql query \e
    'SELECT word FROM
 bigquery.table.`my\-project`.input_dataset.input_table where count
 > 3' \-\-job\-name=my\-job \-\-region=us\-west1 \e
    \-\-bigquery\-dataset=my_output_dataset \e
    \-\-bigquery\-table=my_output_table
.RE

To execute a simple SQL query on Dataflow that reads from and writes to Cloud
Pub/Sub, run:

.RS 2m
$ gcloud dataflow sql query \e
    'SELECT word FROM pubsub.topic.`my\-project`.input_topic where
 count > 3' \-\-job\-name=my\-job \-\-region=us\-west1 \e
    \-\-pubsub\-topic=my_output_topic
.RE

To join data from BigQuery and Cloud Pub/Sub and write the result to Cloud
Pub/Sub, run:

.RS 2m
$ gcloud dataflow sql query \e
    'SELECT bq.name AS name FROM
 pubsub.topic.`my\-project`.input_topic p INNER JOIN
 bigquery.table.`my\-project`.input_dataset.input_table bq ON p.id =
 bq.id' \-\-job\-name=my\-job \-\-region=us\-west1 \e
    \-\-pubsub\-topic=my_output_topic
.RE

To execute a parameterized SQL query that reads from and writes to BigQuery,
run:

.RS 2m
$ gcloud dataflow sql query \e
    'SELECT word FROM
 bigquery.table.`my\-project`.input_dataset.input_table where count
 > @threshold' \-\-parameter=threshold:INT64:5 \-\-job\-name=my\-job \e
    \-\-region=us\-west1 \-\-bigquery\-dataset=my_output_dataset \e
    \-\-bigquery\-table=my_output_table
.RE



.SH "NOTES"

This variant is also available:

.RS 2m
$ gcloud beta dataflow sql query
.RE

